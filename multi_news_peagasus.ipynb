{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Pegasus-Large Summarization ‚Äî MultiNews (Session-only ZIP)\n",
        "# =============================================================\n",
        "\n",
        "!pip install transformers datasets rouge-score accelerate --quiet\n",
        "\n",
        "import torch, gc, os, pandas as pd, sys, zipfile\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "# =============================================================\n",
        "# Config (ONLY paths + dataset adapted)\n",
        "# =============================================================\n",
        "MODEL_NAME = \"google/pegasus-large\"\n",
        "BATCH_SIZE = 10\n",
        "CHUNK_SIZE = 200\n",
        "LIMIT = 15000\n",
        "SPLIT = \"val\"\n",
        "\n",
        "ZIP_PATH = \"/content/multi_news.zip\"              # üëà already in session\n",
        "EXTRACT_DIR = \"/content/data_multinews\"\n",
        "SAVE_PATH = f\"/content/summaries_pegasus_multinews_{SPLIT}.csv\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"‚úÖ Using device:\", device)\n",
        "\n",
        "# =============================================================\n",
        "# Load model + tokenizer safely\n",
        "# =============================================================\n",
        "torch.cuda.empty_cache(); gc.collect()\n",
        "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "try:\n",
        "    model = PegasusForConditionalGeneration.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        low_cpu_mem_usage=True\n",
        "    ).to(device)\n",
        "except RuntimeError:\n",
        "    print(\"‚ö†Ô∏è GPU OOM during model load ‚Üí switching to CPU\")\n",
        "    model = PegasusForConditionalGeneration.from_pretrained(MODEL_NAME).to(\"cpu\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# =============================================================\n",
        "# Load MultiNews from ZIP (session only)\n",
        "# =============================================================\n",
        "if not os.path.exists(EXTRACT_DIR):\n",
        "    print(\"üì¶ Extracting MultiNews ZIP...\")\n",
        "    with zipfile.ZipFile(ZIP_PATH, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(EXTRACT_DIR)\n",
        "    print(\"‚úÖ Extraction complete!\")\n",
        "else:\n",
        "    print(\"‚ôªÔ∏è Using already extracted MultiNews folder.\")\n",
        "\n",
        "# Find src + tgt files\n",
        "src_file, tgt_file = None, None\n",
        "for root, _, files in os.walk(EXTRACT_DIR):\n",
        "    for f in files:\n",
        "        if f.startswith(SPLIT) and \"src\" in f and f.endswith(\".cleaned\"):\n",
        "            src_file = os.path.join(root, f)\n",
        "        elif f.startswith(SPLIT) and \"tgt\" in f:\n",
        "            tgt_file = os.path.join(root, f)\n",
        "\n",
        "if not src_file or not tgt_file:\n",
        "    raise ValueError(\"‚ùå Could not find MultiNews source/target files\")\n",
        "\n",
        "print(f\"‚úÖ Source: {src_file}\")\n",
        "print(f\"‚úÖ Target: {tgt_file}\")\n",
        "\n",
        "with open(src_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    articles = [l.strip() for l in f.readlines()]\n",
        "\n",
        "with open(tgt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    references = [l.strip() for l in f.readlines()]\n",
        "\n",
        "min_len = min(len(articles), len(references))\n",
        "articles = articles[:min_len][:LIMIT]\n",
        "references = references[:min_len][:LIMIT]\n",
        "\n",
        "print(f\"üìö Loaded {len(articles)} MultiNews samples ({SPLIT})\")\n",
        "\n",
        "# =============================================================\n",
        "# Summarization helper (UNCHANGED)\n",
        "# =============================================================\n",
        "def summarize_batch(texts):\n",
        "    inputs = tokenizer(\n",
        "        texts, max_length=1024, truncation=True, padding=True, return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=130,\n",
        "            min_length=30,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    outputs = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
        "    del inputs, summary_ids\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return outputs\n",
        "\n",
        "# =============================================================\n",
        "# Save progress locally (UNCHANGED)\n",
        "# =============================================================\n",
        "def save_progress(articles, refs, summaries, save_path):\n",
        "    df = pd.DataFrame({\n",
        "        \"document\": articles,\n",
        "        \"reference\": refs,\n",
        "        \"summary\": summaries\n",
        "    })\n",
        "    header = not os.path.exists(save_path)\n",
        "    df.to_csv(save_path, mode=\"a\", header=header, index=False)\n",
        "\n",
        "# =============================================================\n",
        "# Resume support (UNCHANGED)\n",
        "# =============================================================\n",
        "start_idx = 0\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    df_prev = pd.read_csv(SAVE_PATH)\n",
        "    start_idx = len(df_prev)\n",
        "    print(f\"üîÅ Resuming from index {start_idx}\")\n",
        "\n",
        "# =============================================================\n",
        "# Main summarization loop (UNCHANGED)\n",
        "# =============================================================\n",
        "total_chunks = (len(articles) - start_idx + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
        "chunk_no = 1\n",
        "\n",
        "for i in range(start_idx, len(articles), CHUNK_SIZE):\n",
        "    end = min(i + CHUNK_SIZE, len(articles))\n",
        "    print(f\"\\nüöÄ Chunk {chunk_no}/{total_chunks} ‚Üí Samples {i}‚Äì{end}\")\n",
        "\n",
        "    batch_articles = articles[i:end]\n",
        "    batch_refs = references[i:end]\n",
        "    all_summaries = []\n",
        "\n",
        "    total_batches = (len(batch_articles) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "    for j in range(0, len(batch_articles), BATCH_SIZE):\n",
        "        sub_batch = batch_articles[j:j + BATCH_SIZE]\n",
        "        batch_no = (j // BATCH_SIZE) + 1\n",
        "\n",
        "        try:\n",
        "            batch_summaries = summarize_batch(sub_batch)\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            torch.cuda.empty_cache(); gc.collect()\n",
        "            batch_summaries = summarize_batch(sub_batch)\n",
        "\n",
        "        all_summaries.extend(batch_summaries)\n",
        "        sys.stdout.write(f\"\\r[Batch {batch_no}/{total_batches}]\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    print()\n",
        "    save_progress(batch_articles, batch_refs, all_summaries, SAVE_PATH)\n",
        "    print(f\"üíæ Saved chunk {chunk_no}/{total_chunks} ({end}/{len(articles)} done)\")\n",
        "\n",
        "    chunk_no += 1\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"\\nüéâ All MultiNews summaries complete!\")\n",
        "print(\"üìÑ Saved to:\", SAVE_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "Cih5J4KJIJXl",
        "outputId": "9a947b9d-cd30-47b2-e059-4b417b12dbad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Extracting MultiNews ZIP...\n",
            "‚úÖ Extraction complete!\n",
            "‚úÖ Source: /content/data_multinews/multi_news/val.src.cleaned\n",
            "‚úÖ Target: /content/data_multinews/multi_news/val.tgt\n",
            "üìö Loaded 5622 MultiNews samples (val)\n",
            "\n",
            "üöÄ Chunk 1/29 ‚Üí Samples 0‚Äì200\n",
            "[Batch 5/20]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3601312263.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mbatch_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfMemoryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3601312263.py\u001b[0m in \u001b[0;36msummarize_batch\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         summary_ids = model.generate(\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3330\u001b[0m             \u001b[0;31m# d. Check which running sequences have finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3331\u001b[0m             next_token_hits_stopping_criteria = stopping_criteria(\n\u001b[0;32m-> 3332\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten_beam_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopk_running_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcur_len\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# remove unfilled token indexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3333\u001b[0m                 \u001b[0mall_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3334\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_flatten_beam_dim\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m   2876\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2878\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2879\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_flatten_beam_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2880\u001b[0m         \u001b[0;34m\"\"\"[batch_size, num_beams, ...] -> [batch_size * num_beams, ...]\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "ixmdLUq2CgXC",
        "outputId": "9d4526d3-0183-4187-cb9c-271e5a4e21e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì 1/200 done\n",
            "‚úì 2/200 done\n",
            "‚úì 3/200 done\n",
            "‚úì 4/200 done\n",
            "‚úì 5/200 done\n",
            "‚úì 6/200 done\n",
            "‚úì 7/200 done\n",
            "‚úì 8/200 done\n",
            "‚úì 9/200 done\n",
            "‚úì 10/200 done\n",
            "‚úì 11/200 done\n",
            "‚úì 12/200 done\n",
            "‚úì 13/200 done\n",
            "‚úì 14/200 done\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-72096694.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Stage 1: document-level summaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mdoc_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Stage 2: fusion summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-72096694.py\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(texts, max_len, min_len)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         ids = model.generate(\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3263\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_running_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3265\u001b[0;31m             \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3267\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1468\u001b[0m                 )\n\u001b[1;32m   1469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1470\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1471\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1297\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1118\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1121\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 \u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_values, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# =============================================================\n",
        "# TRUE MULTI-DOCUMENT SUMMARIZATION ‚Äî PEGASUS (SINGLE CELL)\n",
        "# Multi-News | Hierarchical: Doc-level ‚Üí Fusion\n",
        "# =============================================================\n",
        "\n",
        "!pip install transformers rouge-score accelerate --quiet\n",
        "\n",
        "import torch, gc, os, pandas as pd\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "MODEL_NAME = \"google/pegasus-large\"\n",
        "DATA_DIR = \"/content/data_multinews\"\n",
        "SPLIT = \"val\"\n",
        "LIMIT = 200\n",
        "SAVE_PATH = f\"/content/pegasus_multidoc_{SPLIT}.csv\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ").to(device)\n",
        "model.eval()\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "src_file, tgt_file = None, None\n",
        "for root, _, files in os.walk(DATA_DIR):\n",
        "    for f in files:\n",
        "        if f.startswith(SPLIT) and \"src\" in f:\n",
        "            src_file = os.path.join(root, f)\n",
        "        if f.startswith(SPLIT) and \"tgt\" in f:\n",
        "            tgt_file = os.path.join(root, f)\n",
        "\n",
        "articles = open(src_file).read().splitlines()[:LIMIT]\n",
        "references = open(tgt_file).read().splitlines()[:LIMIT]\n",
        "\n",
        "# ---------------- HELPERS ----------------\n",
        "def split_docs(sample):\n",
        "    return [d.strip() for d in sample.split(\"|||||\") if d.strip()]\n",
        "\n",
        "def summarize(texts, max_len, min_len):\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_len,\n",
        "            min_length=min_len,\n",
        "            num_beams=4\n",
        "        )\n",
        "\n",
        "    out = tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
        "    del inputs, ids\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return out\n",
        "\n",
        "# ---------------- PIPELINE ----------------\n",
        "results = []\n",
        "\n",
        "for i, sample in enumerate(articles):\n",
        "    docs = split_docs(sample)\n",
        "\n",
        "    # Stage 1: document-level summaries\n",
        "    doc_summaries = summarize(docs, max_len=256, min_len=60)\n",
        "\n",
        "    # Stage 2: fusion summary\n",
        "    fused_text = \" \".join(doc_summaries)\n",
        "    final_summary = summarize([fused_text], max_len=150, min_len=60)[0]\n",
        "\n",
        "    results.append({\n",
        "        \"document\": sample,\n",
        "        \"reference\": references[i],\n",
        "        \"summary\": final_summary\n",
        "    })\n",
        "\n",
        "    print(f\"‚úì {i+1}/{len(articles)} done\")\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "# ---------------- SAVE ----------------\n",
        "pd.DataFrame(results).to_csv(SAVE_PATH, index=False)\n",
        "print(\"Saved to:\", SAVE_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# TRUE MULTI-DOCUMENT SUMMARIZATION ‚Äî PEGASUS (RESUMABLE)\n",
        "# Multi-News | Hierarchical: Doc-level ‚Üí Fusion\n",
        "# =============================================================\n",
        "\n",
        "!pip install transformers rouge-score accelerate --quiet\n",
        "\n",
        "import torch, gc, os, pandas as pd\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "MODEL_NAME = \"google/pegasus-large\"\n",
        "DATA_DIR = \"/content/data_multinews\"\n",
        "SPLIT = \"val\"\n",
        "LIMIT = 200\n",
        "SAVE_PATH = f\"/content/pegasus_multidoc_{SPLIT}.csv\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ").to(device)\n",
        "model.eval()\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "src_file, tgt_file = None, None\n",
        "for root, _, files in os.walk(DATA_DIR):\n",
        "    for f in files:\n",
        "        if f.startswith(SPLIT) and \"src\" in f:\n",
        "            src_file = os.path.join(root, f)\n",
        "        if f.startswith(SPLIT) and \"tgt\" in f:\n",
        "            tgt_file = os.path.join(root, f)\n",
        "\n",
        "articles = open(src_file).read().splitlines()[:LIMIT]\n",
        "references = open(tgt_file).read().splitlines()[:LIMIT]\n",
        "\n",
        "# ---------------- RESUME SUPPORT ----------------\n",
        "start_idx = 0\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    prev_df = pd.read_csv(SAVE_PATH)\n",
        "    start_idx = len(prev_df)\n",
        "    print(f\"üîÅ Resuming from index {start_idx}\")\n",
        "\n",
        "# ---------------- HELPERS ----------------\n",
        "def split_docs(sample):\n",
        "    return [d.strip() for d in sample.split(\"|||||\") if d.strip()]\n",
        "\n",
        "def summarize(texts, max_len, min_len):\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_len,\n",
        "            min_length=min_len,\n",
        "            num_beams=4\n",
        "        )\n",
        "\n",
        "    out = tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
        "    del inputs, ids\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return out\n",
        "\n",
        "# ---------------- PIPELINE ----------------\n",
        "results = []\n",
        "\n",
        "for i in range(start_idx, len(articles)):\n",
        "    sample = articles[i]\n",
        "    docs = split_docs(sample)\n",
        "\n",
        "    # Stage 1: document-level summaries\n",
        "    doc_summaries = summarize(docs, max_len=256, min_len=60)\n",
        "\n",
        "    # Stage 2: fusion summary\n",
        "    fused_text = \" \".join(doc_summaries)\n",
        "    final_summary = summarize([fused_text], max_len=150, min_len=60)[0]\n",
        "\n",
        "    results.append({\n",
        "        \"document\": sample,\n",
        "        \"reference\": references[i],\n",
        "        \"summary\": final_summary\n",
        "    })\n",
        "\n",
        "    print(f\"‚úì {i+1}/{len(articles)} done\")\n",
        "\n",
        "    # save every sample (safe for Colab crashes)\n",
        "    pd.DataFrame(results).to_csv(\n",
        "        SAVE_PATH,\n",
        "        mode=\"a\",\n",
        "        header=not os.path.exists(SAVE_PATH),\n",
        "        index=False\n",
        "    )\n",
        "    results.clear()\n",
        "\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"‚úÖ Summarization complete. Saved to:\", SAVE_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dcI9AhII7BM",
        "outputId": "8359bd52-9366-4576-bf1c-d9f98746a3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Resuming from index 200\n",
            "‚úÖ Summarization complete. Saved to: /content/pegasus_multidoc_val.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# TRUE MULTI-DOCUMENT SUMMARIZATION ‚Äî PEGASUS (RESUMABLE)\n",
        "# Multi-News | Hierarchical: Doc-level ‚Üí Fusion\n",
        "# =============================================================\n",
        "\n",
        "!pip install transformers rouge-score accelerate --quiet\n",
        "\n",
        "import torch, gc, os, pandas as pd\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "MODEL_NAME = \"google/pegasus-large\"\n",
        "DATA_DIR = \"/content/data_multinews\"\n",
        "SPLIT = \"val\"\n",
        "LIMIT = 400\n",
        "SAVE_PATH = f\"/content/pegasus_multidoc_{SPLIT}.csv\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ").to(device)\n",
        "model.eval()\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "src_file, tgt_file = None, None\n",
        "for root, _, files in os.walk(DATA_DIR):\n",
        "    for f in files:\n",
        "        if f.startswith(SPLIT) and \"src\" in f:\n",
        "            src_file = os.path.join(root, f)\n",
        "        if f.startswith(SPLIT) and \"tgt\" in f:\n",
        "            tgt_file = os.path.join(root, f)\n",
        "\n",
        "articles = open(src_file).read().splitlines()[:LIMIT]\n",
        "references = open(tgt_file).read().splitlines()[:LIMIT]\n",
        "\n",
        "# ---------------- RESUME SUPPORT ----------------\n",
        "start_idx = 0\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    prev_df = pd.read_csv(SAVE_PATH)\n",
        "    start_idx = len(prev_df)\n",
        "    print(f\"üîÅ Resuming from index {start_idx}\")\n",
        "\n",
        "# ---------------- HELPERS ----------------\n",
        "def split_docs(sample):\n",
        "    return [d.strip() for d in sample.split(\"|||||\") if d.strip()]\n",
        "\n",
        "def summarize(texts, max_len, min_len):\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_len,\n",
        "            min_length=min_len,\n",
        "            num_beams=4\n",
        "        )\n",
        "\n",
        "    out = tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
        "    del inputs, ids\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return out\n",
        "\n",
        "# ---------------- PIPELINE ----------------\n",
        "results = []\n",
        "\n",
        "for i in range(start_idx, len(articles)):\n",
        "    sample = articles[i]\n",
        "    docs = split_docs(sample)\n",
        "\n",
        "    # Stage 1: document-level summaries\n",
        "    doc_summaries = summarize(docs, max_len=256, min_len=60)\n",
        "\n",
        "    # Stage 2: fusion summary\n",
        "    fused_text = \" \".join(doc_summaries)\n",
        "    final_summary = summarize([fused_text], max_len=150, min_len=60)[0]\n",
        "\n",
        "    results.append({\n",
        "        \"document\": sample,\n",
        "        \"reference\": references[i],\n",
        "        \"summary\": final_summary\n",
        "    })\n",
        "\n",
        "    print(f\"‚úì {i+1}/{len(articles)} done\")\n",
        "\n",
        "    # save every sample (safe for Colab crashes)\n",
        "    pd.DataFrame(results).to_csv(\n",
        "        SAVE_PATH,\n",
        "        mode=\"a\",\n",
        "        header=not os.path.exists(SAVE_PATH),\n",
        "        index=False\n",
        "    )\n",
        "    results.clear()\n",
        "\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"‚úÖ Summarization complete. Saved to:\", SAVE_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wfl7yEU6J8Py",
        "outputId": "3d7ca382-649e-4a30-97b4-4af7788b4f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Resuming from index 200\n",
            "‚úì 201/400 done\n",
            "‚úì 202/400 done\n",
            "‚úì 203/400 done\n",
            "‚úì 204/400 done\n",
            "‚úì 205/400 done\n",
            "‚úì 206/400 done\n",
            "‚úì 207/400 done\n",
            "‚úì 208/400 done\n",
            "‚úì 209/400 done\n",
            "‚úì 210/400 done\n",
            "‚úì 211/400 done\n",
            "‚úì 212/400 done\n",
            "‚úì 213/400 done\n",
            "‚úì 214/400 done\n",
            "‚úì 215/400 done\n",
            "‚úì 216/400 done\n",
            "‚úì 217/400 done\n",
            "‚úì 218/400 done\n",
            "‚úì 219/400 done\n",
            "‚úì 220/400 done\n",
            "‚úì 221/400 done\n",
            "‚úì 222/400 done\n",
            "‚úì 223/400 done\n",
            "‚úì 224/400 done\n",
            "‚úì 225/400 done\n",
            "‚úì 226/400 done\n",
            "‚úì 227/400 done\n",
            "‚úì 228/400 done\n",
            "‚úì 229/400 done\n",
            "‚úì 230/400 done\n",
            "‚úì 231/400 done\n",
            "‚úì 232/400 done\n",
            "‚úì 233/400 done\n",
            "‚úì 234/400 done\n",
            "‚úì 235/400 done\n",
            "‚úì 236/400 done\n",
            "‚úì 237/400 done\n",
            "‚úì 238/400 done\n",
            "‚úì 239/400 done\n",
            "‚úì 240/400 done\n",
            "‚úì 241/400 done\n",
            "‚úì 242/400 done\n",
            "‚úì 243/400 done\n",
            "‚úì 244/400 done\n",
            "‚úì 245/400 done\n",
            "‚úì 246/400 done\n",
            "‚úì 247/400 done\n",
            "‚úì 248/400 done\n",
            "‚úì 249/400 done\n",
            "‚úì 250/400 done\n",
            "‚úì 251/400 done\n",
            "‚úì 252/400 done\n",
            "‚úì 253/400 done\n",
            "‚úì 254/400 done\n",
            "‚úì 255/400 done\n",
            "‚úì 256/400 done\n",
            "‚úì 257/400 done\n",
            "‚úì 258/400 done\n",
            "‚úì 259/400 done\n",
            "‚úì 260/400 done\n",
            "‚úì 261/400 done\n",
            "‚úì 262/400 done\n",
            "‚úì 263/400 done\n",
            "‚úì 264/400 done\n",
            "‚úì 265/400 done\n",
            "‚úì 266/400 done\n",
            "‚úì 267/400 done\n",
            "‚úì 268/400 done\n",
            "‚úì 269/400 done\n",
            "‚úì 270/400 done\n",
            "‚úì 271/400 done\n",
            "‚úì 272/400 done\n",
            "‚úì 273/400 done\n",
            "‚úì 274/400 done\n",
            "‚úì 275/400 done\n",
            "‚úì 276/400 done\n",
            "‚úì 277/400 done\n",
            "‚úì 278/400 done\n",
            "‚úì 279/400 done\n",
            "‚úì 280/400 done\n",
            "‚úì 281/400 done\n",
            "‚úì 282/400 done\n",
            "‚úì 283/400 done\n",
            "‚úì 284/400 done\n",
            "‚úì 285/400 done\n",
            "‚úì 286/400 done\n",
            "‚úì 287/400 done\n",
            "‚úì 288/400 done\n",
            "‚úì 289/400 done\n",
            "‚úì 290/400 done\n",
            "‚úì 291/400 done\n",
            "‚úì 292/400 done\n",
            "‚úì 293/400 done\n",
            "‚úì 294/400 done\n",
            "‚úì 295/400 done\n",
            "‚úì 296/400 done\n",
            "‚úì 297/400 done\n",
            "‚úì 298/400 done\n",
            "‚úì 299/400 done\n",
            "‚úì 300/400 done\n",
            "‚úì 301/400 done\n",
            "‚úì 302/400 done\n",
            "‚úì 303/400 done\n",
            "‚úì 304/400 done\n",
            "‚úì 305/400 done\n",
            "‚úì 306/400 done\n",
            "‚úì 307/400 done\n",
            "‚úì 308/400 done\n",
            "‚úì 309/400 done\n",
            "‚úì 310/400 done\n",
            "‚úì 311/400 done\n",
            "‚úì 312/400 done\n",
            "‚úì 313/400 done\n",
            "‚úì 314/400 done\n",
            "‚úì 315/400 done\n",
            "‚úì 316/400 done\n",
            "‚úì 317/400 done\n",
            "‚úì 318/400 done\n",
            "‚úì 319/400 done\n",
            "‚úì 320/400 done\n",
            "‚úì 321/400 done\n",
            "‚úì 322/400 done\n",
            "‚úì 323/400 done\n",
            "‚úì 324/400 done\n",
            "‚úì 325/400 done\n",
            "‚úì 326/400 done\n",
            "‚úì 327/400 done\n",
            "‚úì 328/400 done\n",
            "‚úì 329/400 done\n",
            "‚úì 330/400 done\n",
            "‚úì 331/400 done\n",
            "‚úì 332/400 done\n",
            "‚úì 333/400 done\n",
            "‚úì 334/400 done\n",
            "‚úì 335/400 done\n",
            "‚úì 336/400 done\n",
            "‚úì 337/400 done\n",
            "‚úì 338/400 done\n",
            "‚úì 339/400 done\n",
            "‚úì 340/400 done\n",
            "‚úì 341/400 done\n",
            "‚úì 342/400 done\n",
            "‚úì 343/400 done\n",
            "‚úì 344/400 done\n",
            "‚úì 345/400 done\n",
            "‚úì 346/400 done\n",
            "‚úì 347/400 done\n",
            "‚úì 348/400 done\n",
            "‚úì 349/400 done\n",
            "‚úì 350/400 done\n",
            "‚úì 351/400 done\n",
            "‚úì 352/400 done\n",
            "‚úì 353/400 done\n",
            "‚úì 354/400 done\n",
            "‚úì 355/400 done\n",
            "‚úì 356/400 done\n",
            "‚úì 357/400 done\n",
            "‚úì 358/400 done\n",
            "‚úì 359/400 done\n",
            "‚úì 360/400 done\n",
            "‚úì 361/400 done\n",
            "‚úì 362/400 done\n",
            "‚úì 363/400 done\n",
            "‚úì 364/400 done\n",
            "‚úì 365/400 done\n",
            "‚úì 366/400 done\n",
            "‚úì 367/400 done\n",
            "‚úì 368/400 done\n",
            "‚úì 369/400 done\n",
            "‚úì 370/400 done\n",
            "‚úì 371/400 done\n",
            "‚úì 372/400 done\n",
            "‚úì 373/400 done\n",
            "‚úì 374/400 done\n",
            "‚úì 375/400 done\n",
            "‚úì 376/400 done\n",
            "‚úì 377/400 done\n",
            "‚úì 378/400 done\n",
            "‚úì 379/400 done\n",
            "‚úì 380/400 done\n",
            "‚úì 381/400 done\n",
            "‚úì 382/400 done\n",
            "‚úì 383/400 done\n",
            "‚úì 384/400 done\n",
            "‚úì 385/400 done\n",
            "‚úì 386/400 done\n",
            "‚úì 387/400 done\n",
            "‚úì 388/400 done\n",
            "‚úì 389/400 done\n",
            "‚úì 390/400 done\n",
            "‚úì 391/400 done\n",
            "‚úì 392/400 done\n",
            "‚úì 393/400 done\n",
            "‚úì 394/400 done\n",
            "‚úì 395/400 done\n",
            "‚úì 396/400 done\n",
            "‚úì 397/400 done\n",
            "‚úì 398/400 done\n",
            "‚úì 399/400 done\n",
            "‚úì 400/400 done\n",
            "‚úÖ Summarization complete. Saved to: /content/pegasus_multidoc_val.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# TRUE MULTI-DOCUMENT SUMMARIZATION ‚Äî PEGASUS (RESUMABLE)\n",
        "# Multi-News | Hierarchical: Doc-level ‚Üí Fusion\n",
        "# =============================================================\n",
        "\n",
        "!pip install transformers rouge-score accelerate --quiet\n",
        "\n",
        "import torch, gc, os, pandas as pd\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "MODEL_NAME = \"google/pegasus-large\"\n",
        "DATA_DIR = \"/content/data_multinews\"\n",
        "SPLIT = \"val\"\n",
        "LIMIT = 800\n",
        "SAVE_PATH = f\"/content/pegasus_multidoc_{SPLIT}.csv\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ").to(device)\n",
        "model.eval()\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "src_file, tgt_file = None, None\n",
        "for root, _, files in os.walk(DATA_DIR):\n",
        "    for f in files:\n",
        "        if f.startswith(SPLIT) and \"src\" in f:\n",
        "            src_file = os.path.join(root, f)\n",
        "        if f.startswith(SPLIT) and \"tgt\" in f:\n",
        "            tgt_file = os.path.join(root, f)\n",
        "\n",
        "articles = open(src_file).read().splitlines()[:LIMIT]\n",
        "references = open(tgt_file).read().splitlines()[:LIMIT]\n",
        "\n",
        "# ---------------- RESUME SUPPORT ----------------\n",
        "start_idx = 0\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    prev_df = pd.read_csv(SAVE_PATH)\n",
        "    start_idx = len(prev_df)\n",
        "    print(f\"üîÅ Resuming from index {start_idx}\")\n",
        "\n",
        "# ---------------- HELPERS ----------------\n",
        "def split_docs(sample):\n",
        "    return [d.strip() for d in sample.split(\"|||||\") if d.strip()]\n",
        "\n",
        "def summarize(texts, max_len, min_len):\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_len,\n",
        "            min_length=min_len,\n",
        "            num_beams=4\n",
        "        )\n",
        "\n",
        "    out = tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
        "    del inputs, ids\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return out\n",
        "\n",
        "# ---------------- PIPELINE ----------------\n",
        "results = []\n",
        "\n",
        "for i in range(start_idx, len(articles)):\n",
        "    sample = articles[i]\n",
        "    docs = split_docs(sample)\n",
        "\n",
        "    # Stage 1: document-level summaries\n",
        "    doc_summaries = summarize(docs, max_len=256, min_len=60)\n",
        "\n",
        "    # Stage 2: fusion summary\n",
        "    fused_text = \" \".join(doc_summaries)\n",
        "    final_summary = summarize([fused_text], max_len=150, min_len=60)[0]\n",
        "\n",
        "    results.append({\n",
        "        \"document\": sample,\n",
        "        \"reference\": references[i],\n",
        "        \"summary\": final_summary\n",
        "    })\n",
        "\n",
        "    print(f\"‚úì {i+1}/{len(articles)} done\")\n",
        "\n",
        "    # save every sample (safe for Colab crashes)\n",
        "    pd.DataFrame(results).to_csv(\n",
        "        SAVE_PATH,\n",
        "        mode=\"a\",\n",
        "        header=not os.path.exists(SAVE_PATH),\n",
        "        index=False\n",
        "    )\n",
        "    results.clear()\n",
        "\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"‚úÖ Summarization complete. Saved to:\", SAVE_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmAtQ-1rSTu_",
        "outputId": "78018201-db54-4cc7-d267-7e63cbf64539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Resuming from index 400\n",
            "‚úì 401/800 done\n",
            "‚úì 402/800 done\n",
            "‚úì 403/800 done\n",
            "‚úì 404/800 done\n",
            "‚úì 405/800 done\n",
            "‚úì 406/800 done\n",
            "‚úì 407/800 done\n",
            "‚úì 408/800 done\n",
            "‚úì 409/800 done\n",
            "‚úì 410/800 done\n",
            "‚úì 411/800 done\n",
            "‚úì 412/800 done\n",
            "‚úì 413/800 done\n",
            "‚úì 414/800 done\n",
            "‚úì 415/800 done\n",
            "‚úì 416/800 done\n",
            "‚úì 417/800 done\n",
            "‚úì 418/800 done\n",
            "‚úì 419/800 done\n",
            "‚úì 420/800 done\n",
            "‚úì 421/800 done\n",
            "‚úì 422/800 done\n",
            "‚úì 423/800 done\n",
            "‚úì 424/800 done\n",
            "‚úì 425/800 done\n",
            "‚úì 426/800 done\n",
            "‚úì 427/800 done\n",
            "‚úì 428/800 done\n",
            "‚úì 429/800 done\n",
            "‚úì 430/800 done\n",
            "‚úì 431/800 done\n",
            "‚úì 432/800 done\n",
            "‚úì 433/800 done\n",
            "‚úì 434/800 done\n",
            "‚úì 435/800 done\n",
            "‚úì 436/800 done\n",
            "‚úì 437/800 done\n",
            "‚úì 438/800 done\n",
            "‚úì 439/800 done\n",
            "‚úì 440/800 done\n",
            "‚úì 441/800 done\n",
            "‚úì 442/800 done\n",
            "‚úì 443/800 done\n",
            "‚úì 444/800 done\n",
            "‚úì 445/800 done\n",
            "‚úì 446/800 done\n",
            "‚úì 447/800 done\n",
            "‚úì 448/800 done\n",
            "‚úì 449/800 done\n",
            "‚úì 450/800 done\n",
            "‚úì 451/800 done\n",
            "‚úì 452/800 done\n",
            "‚úì 453/800 done\n",
            "‚úì 454/800 done\n",
            "‚úì 455/800 done\n",
            "‚úì 456/800 done\n",
            "‚úì 457/800 done\n",
            "‚úì 458/800 done\n",
            "‚úì 459/800 done\n",
            "‚úì 460/800 done\n",
            "‚úì 461/800 done\n",
            "‚úì 462/800 done\n",
            "‚úì 463/800 done\n",
            "‚úì 464/800 done\n",
            "‚úì 465/800 done\n",
            "‚úì 466/800 done\n",
            "‚úì 467/800 done\n",
            "‚úì 468/800 done\n",
            "‚úì 469/800 done\n",
            "‚úì 470/800 done\n",
            "‚úì 471/800 done\n",
            "‚úì 472/800 done\n",
            "‚úì 473/800 done\n",
            "‚úì 474/800 done\n",
            "‚úì 475/800 done\n",
            "‚úì 476/800 done\n",
            "‚úì 477/800 done\n",
            "‚úì 478/800 done\n",
            "‚úì 479/800 done\n",
            "‚úì 480/800 done\n",
            "‚úì 481/800 done\n",
            "‚úì 482/800 done\n",
            "‚úì 483/800 done\n",
            "‚úì 484/800 done\n",
            "‚úì 485/800 done\n",
            "‚úì 486/800 done\n",
            "‚úì 487/800 done\n",
            "‚úì 488/800 done\n",
            "‚úì 489/800 done\n",
            "‚úì 490/800 done\n",
            "‚úì 491/800 done\n",
            "‚úì 492/800 done\n",
            "‚úì 493/800 done\n",
            "‚úì 494/800 done\n",
            "‚úì 495/800 done\n",
            "‚úì 496/800 done\n",
            "‚úì 497/800 done\n",
            "‚úì 498/800 done\n",
            "‚úì 499/800 done\n",
            "‚úì 500/800 done\n",
            "‚úì 501/800 done\n",
            "‚úì 502/800 done\n",
            "‚úì 503/800 done\n",
            "‚úì 504/800 done\n",
            "‚úì 505/800 done\n",
            "‚úì 506/800 done\n",
            "‚úì 507/800 done\n",
            "‚úì 508/800 done\n",
            "‚úì 509/800 done\n",
            "‚úì 510/800 done\n",
            "‚úì 511/800 done\n",
            "‚úì 512/800 done\n",
            "‚úì 513/800 done\n",
            "‚úì 514/800 done\n",
            "‚úì 515/800 done\n",
            "‚úì 516/800 done\n",
            "‚úì 517/800 done\n",
            "‚úì 518/800 done\n",
            "‚úì 519/800 done\n",
            "‚úì 520/800 done\n",
            "‚úì 521/800 done\n",
            "‚úì 522/800 done\n",
            "‚úì 523/800 done\n",
            "‚úì 524/800 done\n",
            "‚úì 525/800 done\n",
            "‚úì 526/800 done\n",
            "‚úì 527/800 done\n",
            "‚úì 528/800 done\n",
            "‚úì 529/800 done\n",
            "‚úì 530/800 done\n",
            "‚úì 531/800 done\n",
            "‚úì 532/800 done\n",
            "‚úì 533/800 done\n",
            "‚úì 534/800 done\n",
            "‚úì 535/800 done\n",
            "‚úì 536/800 done\n",
            "‚úì 537/800 done\n",
            "‚úì 538/800 done\n",
            "‚úì 539/800 done\n",
            "‚úì 540/800 done\n",
            "‚úì 541/800 done\n",
            "‚úì 542/800 done\n",
            "‚úì 543/800 done\n",
            "‚úì 544/800 done\n",
            "‚úì 545/800 done\n",
            "‚úì 546/800 done\n",
            "‚úì 547/800 done\n",
            "‚úì 548/800 done\n",
            "‚úì 549/800 done\n",
            "‚úì 550/800 done\n",
            "‚úì 551/800 done\n",
            "‚úì 552/800 done\n",
            "‚úì 553/800 done\n",
            "‚úì 554/800 done\n",
            "‚úì 555/800 done\n",
            "‚úì 556/800 done\n",
            "‚úì 557/800 done\n",
            "‚úì 558/800 done\n",
            "‚úì 559/800 done\n",
            "‚úì 560/800 done\n",
            "‚úì 561/800 done\n",
            "‚úì 562/800 done\n",
            "‚úì 563/800 done\n",
            "‚úì 564/800 done\n",
            "‚úì 565/800 done\n",
            "‚úì 566/800 done\n",
            "‚úì 567/800 done\n",
            "‚úì 568/800 done\n",
            "‚úì 569/800 done\n",
            "‚úì 570/800 done\n",
            "‚úì 571/800 done\n",
            "‚úì 572/800 done\n",
            "‚úì 573/800 done\n",
            "‚úì 574/800 done\n",
            "‚úì 575/800 done\n",
            "‚úì 576/800 done\n",
            "‚úì 577/800 done\n",
            "‚úì 578/800 done\n",
            "‚úì 579/800 done\n",
            "‚úì 580/800 done\n",
            "‚úì 581/800 done\n",
            "‚úì 582/800 done\n",
            "‚úì 583/800 done\n",
            "‚úì 584/800 done\n",
            "‚úì 585/800 done\n",
            "‚úì 586/800 done\n",
            "‚úì 587/800 done\n",
            "‚úì 588/800 done\n",
            "‚úì 589/800 done\n",
            "‚úì 590/800 done\n",
            "‚úì 591/800 done\n",
            "‚úì 592/800 done\n",
            "‚úì 593/800 done\n",
            "‚úì 594/800 done\n",
            "‚úì 595/800 done\n",
            "‚úì 596/800 done\n",
            "‚úì 597/800 done\n",
            "‚úì 598/800 done\n",
            "‚úì 599/800 done\n",
            "‚úì 600/800 done\n",
            "‚úì 601/800 done\n",
            "‚úì 602/800 done\n",
            "‚úì 603/800 done\n",
            "‚úì 604/800 done\n",
            "‚úì 605/800 done\n",
            "‚úì 606/800 done\n",
            "‚úì 607/800 done\n",
            "‚úì 608/800 done\n",
            "‚úì 609/800 done\n",
            "‚úì 610/800 done\n",
            "‚úì 611/800 done\n",
            "‚úì 612/800 done\n",
            "‚úì 613/800 done\n",
            "‚úì 614/800 done\n",
            "‚úì 615/800 done\n",
            "‚úì 616/800 done\n",
            "‚úì 617/800 done\n",
            "‚úì 618/800 done\n",
            "‚úì 619/800 done\n",
            "‚úì 620/800 done\n",
            "‚úì 621/800 done\n",
            "‚úì 622/800 done\n",
            "‚úì 623/800 done\n",
            "‚úì 624/800 done\n",
            "‚úì 625/800 done\n",
            "‚úì 626/800 done\n",
            "‚úì 627/800 done\n",
            "‚úì 628/800 done\n",
            "‚úì 629/800 done\n",
            "‚úì 630/800 done\n",
            "‚úì 631/800 done\n",
            "‚úì 632/800 done\n",
            "‚úì 633/800 done\n",
            "‚úì 634/800 done\n",
            "‚úì 635/800 done\n",
            "‚úì 636/800 done\n",
            "‚úì 637/800 done\n",
            "‚úì 638/800 done\n",
            "‚úì 639/800 done\n",
            "‚úì 640/800 done\n",
            "‚úì 641/800 done\n",
            "‚úì 642/800 done\n",
            "‚úì 643/800 done\n",
            "‚úì 644/800 done\n",
            "‚úì 645/800 done\n",
            "‚úì 646/800 done\n",
            "‚úì 647/800 done\n",
            "‚úì 648/800 done\n",
            "‚úì 649/800 done\n",
            "‚úì 650/800 done\n",
            "‚úì 651/800 done\n",
            "‚úì 652/800 done\n",
            "‚úì 653/800 done\n",
            "‚úì 654/800 done\n",
            "‚úì 655/800 done\n",
            "‚úì 656/800 done\n",
            "‚úì 657/800 done\n",
            "‚úì 658/800 done\n",
            "‚úì 659/800 done\n",
            "‚úì 660/800 done\n",
            "‚úì 661/800 done\n",
            "‚úì 662/800 done\n",
            "‚úì 663/800 done\n",
            "‚úì 664/800 done\n",
            "‚úì 665/800 done\n",
            "‚úì 666/800 done\n",
            "‚úì 667/800 done\n",
            "‚úì 668/800 done\n",
            "‚úì 669/800 done\n",
            "‚úì 670/800 done\n",
            "‚úì 671/800 done\n",
            "‚úì 672/800 done\n",
            "‚úì 673/800 done\n",
            "‚úì 674/800 done\n",
            "‚úì 675/800 done\n",
            "‚úì 676/800 done\n",
            "‚úì 677/800 done\n",
            "‚úì 678/800 done\n",
            "‚úì 679/800 done\n",
            "‚úì 680/800 done\n",
            "‚úì 681/800 done\n",
            "‚úì 682/800 done\n",
            "‚úì 683/800 done\n",
            "‚úì 684/800 done\n",
            "‚úì 685/800 done\n",
            "‚úì 686/800 done\n",
            "‚úì 687/800 done\n",
            "‚úì 688/800 done\n",
            "‚úì 689/800 done\n",
            "‚úì 690/800 done\n",
            "‚úì 691/800 done\n",
            "‚úì 692/800 done\n",
            "‚úì 693/800 done\n",
            "‚úì 694/800 done\n",
            "‚úì 695/800 done\n",
            "‚úì 696/800 done\n",
            "‚úì 697/800 done\n",
            "‚úì 698/800 done\n",
            "‚úì 699/800 done\n",
            "‚úì 700/800 done\n",
            "‚úì 701/800 done\n",
            "‚úì 702/800 done\n",
            "‚úì 703/800 done\n",
            "‚úì 704/800 done\n",
            "‚úì 705/800 done\n",
            "‚úì 706/800 done\n",
            "‚úì 707/800 done\n",
            "‚úì 708/800 done\n",
            "‚úì 709/800 done\n",
            "‚úì 710/800 done\n",
            "‚úì 711/800 done\n",
            "‚úì 712/800 done\n",
            "‚úì 713/800 done\n",
            "‚úì 714/800 done\n",
            "‚úì 715/800 done\n",
            "‚úì 716/800 done\n",
            "‚úì 717/800 done\n",
            "‚úì 718/800 done\n",
            "‚úì 719/800 done\n",
            "‚úì 720/800 done\n",
            "‚úì 721/800 done\n",
            "‚úì 722/800 done\n",
            "‚úì 723/800 done\n",
            "‚úì 724/800 done\n",
            "‚úì 725/800 done\n",
            "‚úì 726/800 done\n",
            "‚úì 727/800 done\n",
            "‚úì 728/800 done\n",
            "‚úì 729/800 done\n",
            "‚úì 730/800 done\n",
            "‚úì 731/800 done\n",
            "‚úì 732/800 done\n",
            "‚úì 733/800 done\n",
            "‚úì 734/800 done\n",
            "‚úì 735/800 done\n",
            "‚úì 736/800 done\n",
            "‚úì 737/800 done\n",
            "‚úì 738/800 done\n",
            "‚úì 739/800 done\n",
            "‚úì 740/800 done\n",
            "‚úì 741/800 done\n",
            "‚úì 742/800 done\n",
            "‚úì 743/800 done\n",
            "‚úì 744/800 done\n",
            "‚úì 745/800 done\n",
            "‚úì 746/800 done\n",
            "‚úì 747/800 done\n",
            "‚úì 748/800 done\n",
            "‚úì 749/800 done\n",
            "‚úì 750/800 done\n",
            "‚úì 751/800 done\n",
            "‚úì 752/800 done\n",
            "‚úì 753/800 done\n",
            "‚úì 754/800 done\n",
            "‚úì 755/800 done\n",
            "‚úì 756/800 done\n",
            "‚úì 757/800 done\n",
            "‚úì 758/800 done\n",
            "‚úì 759/800 done\n",
            "‚úì 760/800 done\n",
            "‚úì 761/800 done\n",
            "‚úì 762/800 done\n",
            "‚úì 763/800 done\n",
            "‚úì 764/800 done\n",
            "‚úì 765/800 done\n",
            "‚úì 766/800 done\n",
            "‚úì 767/800 done\n",
            "‚úì 768/800 done\n",
            "‚úì 769/800 done\n",
            "‚úì 770/800 done\n",
            "‚úì 771/800 done\n",
            "‚úì 772/800 done\n",
            "‚úì 773/800 done\n",
            "‚úì 774/800 done\n",
            "‚úì 775/800 done\n",
            "‚úì 776/800 done\n",
            "‚úì 777/800 done\n",
            "‚úì 778/800 done\n",
            "‚úì 779/800 done\n",
            "‚úì 780/800 done\n",
            "‚úì 781/800 done\n",
            "‚úì 782/800 done\n",
            "‚úì 783/800 done\n",
            "‚úì 784/800 done\n",
            "‚úì 785/800 done\n",
            "‚úì 786/800 done\n",
            "‚úì 787/800 done\n",
            "‚úì 788/800 done\n",
            "‚úì 789/800 done\n",
            "‚úì 790/800 done\n",
            "‚úì 791/800 done\n",
            "‚úì 792/800 done\n",
            "‚úì 793/800 done\n",
            "‚úì 794/800 done\n",
            "‚úì 795/800 done\n",
            "‚úì 796/800 done\n",
            "‚úì 797/800 done\n",
            "‚úì 798/800 done\n",
            "‚úì 799/800 done\n",
            "‚úì 800/800 done\n",
            "‚úÖ Summarization complete. Saved to: /content/pegasus_multidoc_val.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# TRUE MULTI-DOCUMENT SUMMARIZATION ‚Äî PEGASUS (RESUMABLE)\n",
        "# Multi-News | Hierarchical: Doc-level ‚Üí Fusion\n",
        "# =============================================================\n",
        "\n",
        "!pip install transformers rouge-score accelerate --quiet\n",
        "\n",
        "import torch, gc, os, pandas as pd\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "MODEL_NAME = \"google/pegasus-large\"\n",
        "DATA_DIR = \"/content/data_multinews\"\n",
        "SPLIT = \"val\"\n",
        "LIMIT = 900\n",
        "SAVE_PATH = f\"/content/pegasus_multidoc_{SPLIT}.csv\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ").to(device)\n",
        "model.eval()\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "src_file, tgt_file = None, None\n",
        "for root, _, files in os.walk(DATA_DIR):\n",
        "    for f in files:\n",
        "        if f.startswith(SPLIT) and \"src\" in f:\n",
        "            src_file = os.path.join(root, f)\n",
        "        if f.startswith(SPLIT) and \"tgt\" in f:\n",
        "            tgt_file = os.path.join(root, f)\n",
        "\n",
        "articles = open(src_file).read().splitlines()[:LIMIT]\n",
        "references = open(tgt_file).read().splitlines()[:LIMIT]\n",
        "\n",
        "# ---------------- RESUME SUPPORT ----------------\n",
        "start_idx = 0\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    prev_df = pd.read_csv(SAVE_PATH)\n",
        "    start_idx = len(prev_df)\n",
        "    print(f\"üîÅ Resuming from index {start_idx}\")\n",
        "\n",
        "# ---------------- HELPERS ----------------\n",
        "def split_docs(sample):\n",
        "    return [d.strip() for d in sample.split(\"|||||\") if d.strip()]\n",
        "\n",
        "def summarize(texts, max_len, min_len):\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_len,\n",
        "            min_length=min_len,\n",
        "            num_beams=4\n",
        "        )\n",
        "\n",
        "    out = tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
        "    del inputs, ids\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return out\n",
        "\n",
        "# ---------------- PIPELINE ----------------\n",
        "results = []\n",
        "\n",
        "for i in range(start_idx, len(articles)):\n",
        "    sample = articles[i]\n",
        "    docs = split_docs(sample)\n",
        "\n",
        "    # Stage 1: document-level summaries\n",
        "    doc_summaries = summarize(docs, max_len=256, min_len=60)\n",
        "\n",
        "    # Stage 2: fusion summary\n",
        "    fused_text = \" \".join(doc_summaries)\n",
        "    final_summary = summarize([fused_text], max_len=150, min_len=60)[0]\n",
        "\n",
        "    results.append({\n",
        "        \"document\": sample,\n",
        "        \"reference\": references[i],\n",
        "        \"summary\": final_summary\n",
        "    })\n",
        "\n",
        "    print(f\"‚úì {i+1}/{len(articles)} done\")\n",
        "\n",
        "    # save every sample (safe for Colab crashes)\n",
        "    pd.DataFrame(results).to_csv(\n",
        "        SAVE_PATH,\n",
        "        mode=\"a\",\n",
        "        header=not os.path.exists(SAVE_PATH),\n",
        "        index=False\n",
        "    )\n",
        "    results.clear()\n",
        "\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"‚úÖ Summarization complete. Saved to:\", SAVE_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98gXm4PnhWDk",
        "outputId": "34a6e2d5-d038-49f1-a1ef-5673eb133478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Resuming from index 800\n",
            "‚úì 801/900 done\n",
            "‚úì 802/900 done\n",
            "‚úì 803/900 done\n",
            "‚úì 804/900 done\n",
            "‚úì 805/900 done\n",
            "‚úì 806/900 done\n",
            "‚úì 807/900 done\n",
            "‚úì 808/900 done\n",
            "‚úì 809/900 done\n",
            "‚úì 810/900 done\n",
            "‚úì 811/900 done\n",
            "‚úì 812/900 done\n",
            "‚úì 813/900 done\n",
            "‚úì 814/900 done\n",
            "‚úì 815/900 done\n",
            "‚úì 816/900 done\n",
            "‚úì 817/900 done\n",
            "‚úì 818/900 done\n",
            "‚úì 819/900 done\n",
            "‚úì 820/900 done\n",
            "‚úì 821/900 done\n",
            "‚úì 822/900 done\n",
            "‚úì 823/900 done\n",
            "‚úì 824/900 done\n",
            "‚úì 825/900 done\n",
            "‚úì 826/900 done\n",
            "‚úì 827/900 done\n",
            "‚úì 828/900 done\n",
            "‚úì 829/900 done\n",
            "‚úì 830/900 done\n",
            "‚úì 831/900 done\n",
            "‚úì 832/900 done\n",
            "‚úì 833/900 done\n",
            "‚úì 834/900 done\n",
            "‚úì 835/900 done\n",
            "‚úì 836/900 done\n",
            "‚úì 837/900 done\n",
            "‚úì 838/900 done\n",
            "‚úì 839/900 done\n",
            "‚úì 840/900 done\n",
            "‚úì 841/900 done\n",
            "‚úì 842/900 done\n",
            "‚úì 843/900 done\n",
            "‚úì 844/900 done\n",
            "‚úì 845/900 done\n",
            "‚úì 846/900 done\n",
            "‚úì 847/900 done\n",
            "‚úì 848/900 done\n",
            "‚úì 849/900 done\n",
            "‚úì 850/900 done\n",
            "‚úì 851/900 done\n",
            "‚úì 852/900 done\n",
            "‚úì 853/900 done\n",
            "‚úì 854/900 done\n",
            "‚úì 855/900 done\n",
            "‚úì 856/900 done\n",
            "‚úì 857/900 done\n",
            "‚úì 858/900 done\n",
            "‚úì 859/900 done\n",
            "‚úì 860/900 done\n",
            "‚úì 861/900 done\n",
            "‚úì 862/900 done\n",
            "‚úì 863/900 done\n",
            "‚úì 864/900 done\n",
            "‚úì 865/900 done\n",
            "‚úì 866/900 done\n",
            "‚úì 867/900 done\n",
            "‚úì 868/900 done\n",
            "‚úì 869/900 done\n",
            "‚úì 870/900 done\n",
            "‚úì 871/900 done\n",
            "‚úì 872/900 done\n",
            "‚úì 873/900 done\n",
            "‚úì 874/900 done\n",
            "‚úì 875/900 done\n",
            "‚úì 876/900 done\n",
            "‚úì 877/900 done\n",
            "‚úì 878/900 done\n",
            "‚úì 879/900 done\n",
            "‚úì 880/900 done\n",
            "‚úì 881/900 done\n",
            "‚úì 882/900 done\n",
            "‚úì 883/900 done\n",
            "‚úì 884/900 done\n",
            "‚úì 885/900 done\n",
            "‚úì 886/900 done\n",
            "‚úì 887/900 done\n",
            "‚úì 888/900 done\n",
            "‚úì 889/900 done\n",
            "‚úì 890/900 done\n",
            "‚úì 891/900 done\n",
            "‚úì 892/900 done\n",
            "‚úì 893/900 done\n",
            "‚úì 894/900 done\n",
            "‚úì 895/900 done\n",
            "‚úì 896/900 done\n",
            "‚úì 897/900 done\n",
            "‚úì 898/900 done\n",
            "‚úì 899/900 done\n",
            "‚úì 900/900 done\n",
            "‚úÖ Summarization complete. Saved to: /content/pegasus_multidoc_val.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# TRUE MULTI-DOCUMENT SUMMARIZATION ‚Äî PEGASUS (RESUMABLE)\n",
        "# Multi-News | Hierarchical: Doc-level ‚Üí Fusion\n",
        "# =============================================================\n",
        "\n",
        "!pip install transformers rouge-score accelerate --quiet\n",
        "\n",
        "import torch, gc, os, pandas as pd\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "MODEL_NAME = \"google/pegasus-large\"\n",
        "DATA_DIR = \"/content/data_multinews\"\n",
        "SPLIT = \"val\"\n",
        "LIMIT = 1000\n",
        "SAVE_PATH = f\"/content/pegasus_multidoc_{SPLIT}.csv\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ").to(device)\n",
        "model.eval()\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "src_file, tgt_file = None, None\n",
        "for root, _, files in os.walk(DATA_DIR):\n",
        "    for f in files:\n",
        "        if f.startswith(SPLIT) and \"src\" in f:\n",
        "            src_file = os.path.join(root, f)\n",
        "        if f.startswith(SPLIT) and \"tgt\" in f:\n",
        "            tgt_file = os.path.join(root, f)\n",
        "\n",
        "articles = open(src_file).read().splitlines()[:LIMIT]\n",
        "references = open(tgt_file).read().splitlines()[:LIMIT]\n",
        "\n",
        "# ---------------- RESUME SUPPORT ----------------\n",
        "start_idx = 0\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    prev_df = pd.read_csv(SAVE_PATH)\n",
        "    start_idx = len(prev_df)\n",
        "    print(f\"üîÅ Resuming from index {start_idx}\")\n",
        "\n",
        "# ---------------- HELPERS ----------------\n",
        "def split_docs(sample):\n",
        "    return [d.strip() for d in sample.split(\"|||||\") if d.strip()]\n",
        "\n",
        "def summarize(texts, max_len, min_len):\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_len,\n",
        "            min_length=min_len,\n",
        "            num_beams=4\n",
        "        )\n",
        "\n",
        "    out = tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
        "    del inputs, ids\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return out\n",
        "\n",
        "# ---------------- PIPELINE ----------------\n",
        "results = []\n",
        "\n",
        "for i in range(start_idx, len(articles)):\n",
        "    sample = articles[i]\n",
        "    docs = split_docs(sample)\n",
        "\n",
        "    # Stage 1: document-level summaries\n",
        "    doc_summaries = summarize(docs, max_len=256, min_len=60)\n",
        "\n",
        "    # Stage 2: fusion summary\n",
        "    fused_text = \" \".join(doc_summaries)\n",
        "    final_summary = summarize([fused_text], max_len=150, min_len=60)[0]\n",
        "\n",
        "    results.append({\n",
        "        \"document\": sample,\n",
        "        \"reference\": references[i],\n",
        "        \"summary\": final_summary\n",
        "    })\n",
        "\n",
        "    print(f\"‚úì {i+1}/{len(articles)} done\")\n",
        "\n",
        "    # save every sample (safe for Colab crashes)\n",
        "    pd.DataFrame(results).to_csv(\n",
        "        SAVE_PATH,\n",
        "        mode=\"a\",\n",
        "        header=not os.path.exists(SAVE_PATH),\n",
        "        index=False\n",
        "    )\n",
        "    results.clear()\n",
        "\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"‚úÖ Summarization complete. Saved to:\", SAVE_PATH)\n"
      ],
      "metadata": {
        "id": "44bDDGOblOPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "831ecc22-e182-4773-9715-46d6cfdee3bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Resuming from index 900\n",
            "‚úì 901/1000 done\n",
            "‚úì 902/1000 done\n",
            "‚úì 903/1000 done\n",
            "‚úì 904/1000 done\n",
            "‚úì 905/1000 done\n",
            "‚úì 906/1000 done\n",
            "‚úì 907/1000 done\n",
            "‚úì 908/1000 done\n",
            "‚úì 909/1000 done\n",
            "‚úì 910/1000 done\n",
            "‚úì 911/1000 done\n",
            "‚úì 912/1000 done\n",
            "‚úì 913/1000 done\n",
            "‚úì 914/1000 done\n",
            "‚úì 915/1000 done\n",
            "‚úì 916/1000 done\n",
            "‚úì 917/1000 done\n",
            "‚úì 918/1000 done\n",
            "‚úì 919/1000 done\n",
            "‚úì 920/1000 done\n",
            "‚úì 921/1000 done\n",
            "‚úì 922/1000 done\n",
            "‚úì 923/1000 done\n",
            "‚úì 924/1000 done\n",
            "‚úì 925/1000 done\n",
            "‚úì 926/1000 done\n",
            "‚úì 927/1000 done\n",
            "‚úì 928/1000 done\n",
            "‚úì 929/1000 done\n",
            "‚úì 930/1000 done\n",
            "‚úì 931/1000 done\n",
            "‚úì 932/1000 done\n",
            "‚úì 933/1000 done\n",
            "‚úì 934/1000 done\n",
            "‚úì 935/1000 done\n",
            "‚úì 936/1000 done\n",
            "‚úì 937/1000 done\n",
            "‚úì 938/1000 done\n",
            "‚úì 939/1000 done\n",
            "‚úì 940/1000 done\n",
            "‚úì 941/1000 done\n",
            "‚úì 942/1000 done\n",
            "‚úì 943/1000 done\n",
            "‚úì 944/1000 done\n",
            "‚úì 945/1000 done\n",
            "‚úì 946/1000 done\n",
            "‚úì 947/1000 done\n",
            "‚úì 948/1000 done\n",
            "‚úì 949/1000 done\n",
            "‚úì 950/1000 done\n",
            "‚úì 951/1000 done\n",
            "‚úì 952/1000 done\n",
            "‚úì 953/1000 done\n",
            "‚úì 954/1000 done\n",
            "‚úì 955/1000 done\n",
            "‚úì 956/1000 done\n",
            "‚úì 957/1000 done\n",
            "‚úì 958/1000 done\n",
            "‚úì 959/1000 done\n",
            "‚úì 960/1000 done\n",
            "‚úì 961/1000 done\n",
            "‚úì 962/1000 done\n",
            "‚úì 963/1000 done\n",
            "‚úì 964/1000 done\n",
            "‚úì 965/1000 done\n",
            "‚úì 966/1000 done\n",
            "‚úì 967/1000 done\n",
            "‚úì 968/1000 done\n",
            "‚úì 969/1000 done\n",
            "‚úì 970/1000 done\n",
            "‚úì 971/1000 done\n",
            "‚úì 972/1000 done\n",
            "‚úì 973/1000 done\n",
            "‚úì 974/1000 done\n",
            "‚úì 975/1000 done\n",
            "‚úì 976/1000 done\n",
            "‚úì 977/1000 done\n",
            "‚úì 978/1000 done\n",
            "‚úì 979/1000 done\n",
            "‚úì 980/1000 done\n",
            "‚úì 981/1000 done\n",
            "‚úì 982/1000 done\n",
            "‚úì 983/1000 done\n",
            "‚úì 984/1000 done\n",
            "‚úì 985/1000 done\n",
            "‚úì 986/1000 done\n",
            "‚úì 987/1000 done\n",
            "‚úì 988/1000 done\n",
            "‚úì 989/1000 done\n",
            "‚úì 990/1000 done\n",
            "‚úì 991/1000 done\n",
            "‚úì 992/1000 done\n",
            "‚úì 993/1000 done\n",
            "‚úì 994/1000 done\n",
            "‚úì 995/1000 done\n",
            "‚úì 996/1000 done\n",
            "‚úì 997/1000 done\n",
            "‚úì 998/1000 done\n",
            "‚úì 999/1000 done\n",
            "‚úì 1000/1000 done\n",
            "‚úÖ Summarization complete. Saved to: /content/pegasus_multidoc_val.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "PXCt4ut9MT7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# TRUE MULTI-DOCUMENT SUMMARIZATION ‚Äî PEGASUS (RESUMABLE)\n",
        "# Multi-News | Hierarchical: Doc-level ‚Üí Fusion\n",
        "# =============================================================\n",
        "\n",
        "!pip install transformers rouge-score accelerate --quiet\n",
        "\n",
        "import torch, gc, os, pandas as pd\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "MODEL_NAME = \"google/pegasus-large\"\n",
        "DATA_DIR = \"/content/data_multinews\"\n",
        "SPLIT = \"test\"\n",
        "LIMIT = 200\n",
        "SAVE_PATH = f\"/content/pegasus_multidoc_{SPLIT}.csv\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ").to(device)\n",
        "model.eval()\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "src_file, tgt_file = None, None\n",
        "for root, _, files in os.walk(DATA_DIR):\n",
        "    for f in files:\n",
        "        if f.startswith(SPLIT) and \"src\" in f:\n",
        "            src_file = os.path.join(root, f)\n",
        "        if f.startswith(SPLIT) and \"tgt\" in f:\n",
        "            tgt_file = os.path.join(root, f)\n",
        "\n",
        "articles = open(src_file).read().splitlines()[:LIMIT]\n",
        "references = open(tgt_file).read().splitlines()[:LIMIT]\n",
        "\n",
        "# ---------------- RESUME SUPPORT ----------------\n",
        "start_idx = 0\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    prev_df = pd.read_csv(SAVE_PATH)\n",
        "    start_idx = len(prev_df)\n",
        "    print(f\"üîÅ Resuming from index {start_idx}\")\n",
        "\n",
        "# ---------------- HELPERS ----------------\n",
        "def split_docs(sample):\n",
        "    return [d.strip() for d in sample.split(\"|||||\") if d.strip()]\n",
        "\n",
        "def summarize(texts, max_len, min_len):\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_len,\n",
        "            min_length=min_len,\n",
        "            num_beams=4\n",
        "        )\n",
        "\n",
        "    out = tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
        "    del inputs, ids\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return out\n",
        "\n",
        "# ---------------- PIPELINE ----------------\n",
        "results = []\n",
        "\n",
        "for i in range(start_idx, len(articles)):\n",
        "    sample = articles[i]\n",
        "    docs = split_docs(sample)\n",
        "\n",
        "    # Stage 1: document-level summaries\n",
        "    doc_summaries = summarize(docs, max_len=256, min_len=60)\n",
        "\n",
        "    # Stage 2: fusion summary\n",
        "    fused_text = \" \".join(doc_summaries)\n",
        "    final_summary = summarize([fused_text], max_len=150, min_len=60)[0]\n",
        "\n",
        "    results.append({\n",
        "        \"document\": sample,\n",
        "        \"reference\": references[i],\n",
        "        \"summary\": final_summary\n",
        "    })\n",
        "\n",
        "    print(f\"‚úì {i+1}/{len(articles)} done\")\n",
        "\n",
        "    # save every sample (safe for Colab crashes)\n",
        "    pd.DataFrame(results).to_csv(\n",
        "        SAVE_PATH,\n",
        "        mode=\"a\",\n",
        "        header=not os.path.exists(SAVE_PATH),\n",
        "        index=False\n",
        "    )\n",
        "    results.clear()\n",
        "\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"‚úÖ Summarization complete. Saved to:\", SAVE_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB_F83QSMqj5",
        "outputId": "7dfc2298-8a99-42ee-f7f9-a3246b667719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì 1/200 done\n",
            "‚úì 2/200 done\n",
            "‚úì 3/200 done\n",
            "‚úì 4/200 done\n",
            "‚úì 5/200 done\n",
            "‚úì 6/200 done\n",
            "‚úì 7/200 done\n",
            "‚úì 8/200 done\n",
            "‚úì 9/200 done\n",
            "‚úì 10/200 done\n",
            "‚úì 11/200 done\n",
            "‚úì 12/200 done\n",
            "‚úì 13/200 done\n",
            "‚úì 14/200 done\n",
            "‚úì 15/200 done\n",
            "‚úì 16/200 done\n",
            "‚úì 17/200 done\n",
            "‚úì 18/200 done\n",
            "‚úì 19/200 done\n",
            "‚úì 20/200 done\n",
            "‚úì 21/200 done\n",
            "‚úì 22/200 done\n",
            "‚úì 23/200 done\n",
            "‚úì 24/200 done\n",
            "‚úì 25/200 done\n",
            "‚úì 26/200 done\n",
            "‚úì 27/200 done\n",
            "‚úì 28/200 done\n",
            "‚úì 29/200 done\n",
            "‚úì 30/200 done\n",
            "‚úì 31/200 done\n",
            "‚úì 32/200 done\n",
            "‚úì 33/200 done\n",
            "‚úì 34/200 done\n",
            "‚úì 35/200 done\n",
            "‚úì 36/200 done\n",
            "‚úì 37/200 done\n",
            "‚úì 38/200 done\n",
            "‚úì 39/200 done\n",
            "‚úì 40/200 done\n",
            "‚úì 41/200 done\n",
            "‚úì 42/200 done\n",
            "‚úì 43/200 done\n",
            "‚úì 44/200 done\n",
            "‚úì 45/200 done\n",
            "‚úì 46/200 done\n",
            "‚úì 47/200 done\n",
            "‚úì 48/200 done\n",
            "‚úì 49/200 done\n",
            "‚úì 50/200 done\n",
            "‚úì 51/200 done\n",
            "‚úì 52/200 done\n",
            "‚úì 53/200 done\n",
            "‚úì 54/200 done\n",
            "‚úì 55/200 done\n",
            "‚úì 56/200 done\n",
            "‚úì 57/200 done\n",
            "‚úì 58/200 done\n",
            "‚úì 59/200 done\n",
            "‚úì 60/200 done\n",
            "‚úì 61/200 done\n",
            "‚úì 62/200 done\n",
            "‚úì 63/200 done\n",
            "‚úì 64/200 done\n",
            "‚úì 65/200 done\n",
            "‚úì 66/200 done\n",
            "‚úì 67/200 done\n",
            "‚úì 68/200 done\n",
            "‚úì 69/200 done\n",
            "‚úì 70/200 done\n",
            "‚úì 71/200 done\n",
            "‚úì 72/200 done\n",
            "‚úì 73/200 done\n",
            "‚úì 74/200 done\n",
            "‚úì 75/200 done\n",
            "‚úì 76/200 done\n",
            "‚úì 77/200 done\n",
            "‚úì 78/200 done\n",
            "‚úì 79/200 done\n",
            "‚úì 80/200 done\n",
            "‚úì 81/200 done\n",
            "‚úì 82/200 done\n",
            "‚úì 83/200 done\n",
            "‚úì 84/200 done\n",
            "‚úì 85/200 done\n",
            "‚úì 86/200 done\n",
            "‚úì 87/200 done\n",
            "‚úì 88/200 done\n",
            "‚úì 89/200 done\n",
            "‚úì 90/200 done\n",
            "‚úì 91/200 done\n",
            "‚úì 92/200 done\n",
            "‚úì 93/200 done\n",
            "‚úì 94/200 done\n",
            "‚úì 95/200 done\n",
            "‚úì 96/200 done\n",
            "‚úì 97/200 done\n",
            "‚úì 98/200 done\n",
            "‚úì 99/200 done\n",
            "‚úì 100/200 done\n",
            "‚úì 101/200 done\n",
            "‚úì 102/200 done\n",
            "‚úì 103/200 done\n",
            "‚úì 104/200 done\n",
            "‚úì 105/200 done\n",
            "‚úì 106/200 done\n",
            "‚úì 107/200 done\n",
            "‚úì 108/200 done\n",
            "‚úì 109/200 done\n",
            "‚úì 110/200 done\n",
            "‚úì 111/200 done\n",
            "‚úì 112/200 done\n",
            "‚úì 113/200 done\n",
            "‚úì 114/200 done\n",
            "‚úì 115/200 done\n",
            "‚úì 116/200 done\n",
            "‚úì 117/200 done\n",
            "‚úì 118/200 done\n",
            "‚úì 119/200 done\n",
            "‚úì 120/200 done\n",
            "‚úì 121/200 done\n",
            "‚úì 122/200 done\n",
            "‚úì 123/200 done\n",
            "‚úì 124/200 done\n",
            "‚úì 125/200 done\n",
            "‚úì 126/200 done\n",
            "‚úì 127/200 done\n",
            "‚úì 128/200 done\n",
            "‚úì 129/200 done\n",
            "‚úì 130/200 done\n",
            "‚úì 131/200 done\n",
            "‚úì 132/200 done\n",
            "‚úì 133/200 done\n",
            "‚úì 134/200 done\n",
            "‚úì 135/200 done\n",
            "‚úì 136/200 done\n",
            "‚úì 137/200 done\n",
            "‚úì 138/200 done\n",
            "‚úì 139/200 done\n",
            "‚úì 140/200 done\n",
            "‚úì 141/200 done\n",
            "‚úì 142/200 done\n",
            "‚úì 143/200 done\n",
            "‚úì 144/200 done\n",
            "‚úì 145/200 done\n",
            "‚úì 146/200 done\n",
            "‚úì 147/200 done\n",
            "‚úì 148/200 done\n",
            "‚úì 149/200 done\n",
            "‚úì 150/200 done\n",
            "‚úì 151/200 done\n",
            "‚úì 152/200 done\n",
            "‚úì 153/200 done\n",
            "‚úì 154/200 done\n",
            "‚úì 155/200 done\n",
            "‚úì 156/200 done\n",
            "‚úì 157/200 done\n",
            "‚úì 158/200 done\n",
            "‚úì 159/200 done\n",
            "‚úì 160/200 done\n",
            "‚úì 161/200 done\n",
            "‚úì 162/200 done\n",
            "‚úì 163/200 done\n",
            "‚úì 164/200 done\n",
            "‚úì 165/200 done\n",
            "‚úì 166/200 done\n",
            "‚úì 167/200 done\n",
            "‚úì 168/200 done\n",
            "‚úì 169/200 done\n",
            "‚úì 170/200 done\n",
            "‚úì 171/200 done\n",
            "‚úì 172/200 done\n",
            "‚úì 173/200 done\n",
            "‚úì 174/200 done\n",
            "‚úì 175/200 done\n",
            "‚úì 176/200 done\n",
            "‚úì 177/200 done\n",
            "‚úì 178/200 done\n",
            "‚úì 179/200 done\n",
            "‚úì 180/200 done\n",
            "‚úì 181/200 done\n",
            "‚úì 182/200 done\n",
            "‚úì 183/200 done\n",
            "‚úì 184/200 done\n",
            "‚úì 185/200 done\n",
            "‚úì 186/200 done\n",
            "‚úì 187/200 done\n",
            "‚úì 188/200 done\n",
            "‚úì 189/200 done\n",
            "‚úì 190/200 done\n",
            "‚úì 191/200 done\n",
            "‚úì 192/200 done\n",
            "‚úì 193/200 done\n",
            "‚úì 194/200 done\n",
            "‚úì 195/200 done\n",
            "‚úì 196/200 done\n",
            "‚úì 197/200 done\n",
            "‚úì 198/200 done\n",
            "‚úì 199/200 done\n",
            "‚úì 200/200 done\n",
            "‚úÖ Summarization complete. Saved to: /content/pegasus_multidoc_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# TRUE MULTI-DOCUMENT SUMMARIZATION ‚Äî PEGASUS (RESUMABLE)\n",
        "# Multi-News | Hierarchical: Doc-level ‚Üí Fusion\n",
        "# =============================================================\n",
        "\n",
        "!pip install transformers rouge-score accelerate --quiet\n",
        "\n",
        "import torch, gc, os, pandas as pd\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "MODEL_NAME = \"google/pegasus-large\"\n",
        "DATA_DIR = \"/content/data_multinews\"\n",
        "SPLIT = \"test\"\n",
        "LIMIT = 400\n",
        "SAVE_PATH = f\"/content/pegasus_multidoc_{SPLIT}.csv\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ").to(device)\n",
        "model.eval()\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "src_file, tgt_file = None, None\n",
        "for root, _, files in os.walk(DATA_DIR):\n",
        "    for f in files:\n",
        "        if f.startswith(SPLIT) and \"src\" in f:\n",
        "            src_file = os.path.join(root, f)\n",
        "        if f.startswith(SPLIT) and \"tgt\" in f:\n",
        "            tgt_file = os.path.join(root, f)\n",
        "\n",
        "articles = open(src_file).read().splitlines()[:LIMIT]\n",
        "references = open(tgt_file).read().splitlines()[:LIMIT]\n",
        "\n",
        "# ---------------- RESUME SUPPORT ----------------\n",
        "start_idx = 0\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    prev_df = pd.read_csv(SAVE_PATH)\n",
        "    start_idx = len(prev_df)\n",
        "    print(f\"üîÅ Resuming from index {start_idx}\")\n",
        "\n",
        "# ---------------- HELPERS ----------------\n",
        "def split_docs(sample):\n",
        "    return [d.strip() for d in sample.split(\"|||||\") if d.strip()]\n",
        "\n",
        "def summarize(texts, max_len, min_len):\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_len,\n",
        "            min_length=min_len,\n",
        "            num_beams=4\n",
        "        )\n",
        "\n",
        "    out = tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
        "    del inputs, ids\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return out\n",
        "\n",
        "# ---------------- PIPELINE ----------------\n",
        "results = []\n",
        "\n",
        "for i in range(start_idx, len(articles)):\n",
        "    sample = articles[i]\n",
        "    docs = split_docs(sample)\n",
        "\n",
        "    # Stage 1: document-level summaries\n",
        "    doc_summaries = summarize(docs, max_len=256, min_len=60)\n",
        "\n",
        "    # Stage 2: fusion summary\n",
        "    fused_text = \" \".join(doc_summaries)\n",
        "    final_summary = summarize([fused_text], max_len=150, min_len=60)[0]\n",
        "\n",
        "    results.append({\n",
        "        \"document\": sample,\n",
        "        \"reference\": references[i],\n",
        "        \"summary\": final_summary\n",
        "    })\n",
        "\n",
        "    print(f\"‚úì {i+1}/{len(articles)} done\")\n",
        "\n",
        "    # save every sample (safe for Colab crashes)\n",
        "    pd.DataFrame(results).to_csv(\n",
        "        SAVE_PATH,\n",
        "        mode=\"a\",\n",
        "        header=not os.path.exists(SAVE_PATH),\n",
        "        index=False\n",
        "    )\n",
        "    results.clear()\n",
        "\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"‚úÖ Summarization complete. Saved to:\", SAVE_PATH)\n"
      ],
      "metadata": {
        "id": "ZCh_Qgfol4c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1694982-75bc-45b9-ba9b-ac10d37f7089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Resuming from index 200\n",
            "‚úì 201/400 done\n",
            "‚úì 202/400 done\n",
            "‚úì 203/400 done\n",
            "‚úì 204/400 done\n",
            "‚úì 205/400 done\n",
            "‚úì 206/400 done\n",
            "‚úì 207/400 done\n",
            "‚úì 208/400 done\n",
            "‚úì 209/400 done\n",
            "‚úì 210/400 done\n",
            "‚úì 211/400 done\n",
            "‚úì 212/400 done\n",
            "‚úì 213/400 done\n",
            "‚úì 214/400 done\n",
            "‚úì 215/400 done\n",
            "‚úì 216/400 done\n",
            "‚úì 217/400 done\n",
            "‚úì 218/400 done\n",
            "‚úì 219/400 done\n",
            "‚úì 220/400 done\n",
            "‚úì 221/400 done\n",
            "‚úì 222/400 done\n",
            "‚úì 223/400 done\n",
            "‚úì 224/400 done\n",
            "‚úì 225/400 done\n",
            "‚úì 226/400 done\n",
            "‚úì 227/400 done\n",
            "‚úì 228/400 done\n",
            "‚úì 229/400 done\n",
            "‚úì 230/400 done\n",
            "‚úì 231/400 done\n",
            "‚úì 232/400 done\n",
            "‚úì 233/400 done\n",
            "‚úì 234/400 done\n",
            "‚úì 235/400 done\n",
            "‚úì 236/400 done\n",
            "‚úì 237/400 done\n",
            "‚úì 238/400 done\n",
            "‚úì 239/400 done\n",
            "‚úì 240/400 done\n",
            "‚úì 241/400 done\n",
            "‚úì 242/400 done\n",
            "‚úì 243/400 done\n",
            "‚úì 244/400 done\n",
            "‚úì 245/400 done\n",
            "‚úì 246/400 done\n",
            "‚úì 247/400 done\n",
            "‚úì 248/400 done\n",
            "‚úì 249/400 done\n",
            "‚úì 250/400 done\n",
            "‚úì 251/400 done\n",
            "‚úì 252/400 done\n",
            "‚úì 253/400 done\n",
            "‚úì 254/400 done\n",
            "‚úì 255/400 done\n",
            "‚úì 256/400 done\n",
            "‚úì 257/400 done\n",
            "‚úì 258/400 done\n",
            "‚úì 259/400 done\n",
            "‚úì 260/400 done\n",
            "‚úì 261/400 done\n",
            "‚úì 262/400 done\n",
            "‚úì 263/400 done\n",
            "‚úì 264/400 done\n",
            "‚úì 265/400 done\n",
            "‚úì 266/400 done\n",
            "‚úì 267/400 done\n",
            "‚úì 268/400 done\n",
            "‚úì 269/400 done\n",
            "‚úì 270/400 done\n",
            "‚úì 271/400 done\n",
            "‚úì 272/400 done\n",
            "‚úì 273/400 done\n",
            "‚úì 274/400 done\n",
            "‚úì 275/400 done\n",
            "‚úì 276/400 done\n",
            "‚úì 277/400 done\n",
            "‚úì 278/400 done\n",
            "‚úì 279/400 done\n",
            "‚úì 280/400 done\n",
            "‚úì 281/400 done\n",
            "‚úì 282/400 done\n",
            "‚úì 283/400 done\n",
            "‚úì 284/400 done\n",
            "‚úì 285/400 done\n",
            "‚úì 286/400 done\n",
            "‚úì 287/400 done\n",
            "‚úì 288/400 done\n",
            "‚úì 289/400 done\n",
            "‚úì 290/400 done\n",
            "‚úì 291/400 done\n",
            "‚úì 292/400 done\n",
            "‚úì 293/400 done\n",
            "‚úì 294/400 done\n",
            "‚úì 295/400 done\n",
            "‚úì 296/400 done\n",
            "‚úì 297/400 done\n",
            "‚úì 298/400 done\n",
            "‚úì 299/400 done\n",
            "‚úì 300/400 done\n",
            "‚úì 301/400 done\n",
            "‚úì 302/400 done\n",
            "‚úì 303/400 done\n",
            "‚úì 304/400 done\n",
            "‚úì 305/400 done\n",
            "‚úì 306/400 done\n",
            "‚úì 307/400 done\n",
            "‚úì 308/400 done\n",
            "‚úì 309/400 done\n",
            "‚úì 310/400 done\n",
            "‚úì 311/400 done\n",
            "‚úì 312/400 done\n",
            "‚úì 313/400 done\n",
            "‚úì 314/400 done\n",
            "‚úì 315/400 done\n",
            "‚úì 316/400 done\n",
            "‚úì 317/400 done\n",
            "‚úì 318/400 done\n",
            "‚úì 319/400 done\n",
            "‚úì 320/400 done\n",
            "‚úì 321/400 done\n",
            "‚úì 322/400 done\n",
            "‚úì 323/400 done\n",
            "‚úì 324/400 done\n",
            "‚úì 325/400 done\n",
            "‚úì 326/400 done\n",
            "‚úì 327/400 done\n",
            "‚úì 328/400 done\n",
            "‚úì 329/400 done\n",
            "‚úì 330/400 done\n",
            "‚úì 331/400 done\n",
            "‚úì 332/400 done\n",
            "‚úì 333/400 done\n",
            "‚úì 334/400 done\n",
            "‚úì 335/400 done\n",
            "‚úì 336/400 done\n",
            "‚úì 337/400 done\n",
            "‚úì 338/400 done\n",
            "‚úì 339/400 done\n",
            "‚úì 340/400 done\n",
            "‚úì 341/400 done\n",
            "‚úì 342/400 done\n",
            "‚úì 343/400 done\n",
            "‚úì 344/400 done\n",
            "‚úì 345/400 done\n",
            "‚úì 346/400 done\n",
            "‚úì 347/400 done\n",
            "‚úì 348/400 done\n",
            "‚úì 349/400 done\n",
            "‚úì 350/400 done\n",
            "‚úì 351/400 done\n",
            "‚úì 352/400 done\n",
            "‚úì 353/400 done\n",
            "‚úì 354/400 done\n",
            "‚úì 355/400 done\n",
            "‚úì 356/400 done\n",
            "‚úì 357/400 done\n",
            "‚úì 358/400 done\n",
            "‚úì 359/400 done\n",
            "‚úì 360/400 done\n",
            "‚úì 361/400 done\n",
            "‚úì 362/400 done\n",
            "‚úì 363/400 done\n",
            "‚úì 364/400 done\n",
            "‚úì 365/400 done\n",
            "‚úì 366/400 done\n",
            "‚úì 367/400 done\n",
            "‚úì 368/400 done\n",
            "‚úì 369/400 done\n",
            "‚úì 370/400 done\n",
            "‚úì 371/400 done\n",
            "‚úì 372/400 done\n",
            "‚úì 373/400 done\n",
            "‚úì 374/400 done\n",
            "‚úì 375/400 done\n",
            "‚úì 376/400 done\n",
            "‚úì 377/400 done\n",
            "‚úì 378/400 done\n",
            "‚úì 379/400 done\n",
            "‚úì 380/400 done\n",
            "‚úì 381/400 done\n",
            "‚úì 382/400 done\n",
            "‚úì 383/400 done\n",
            "‚úì 384/400 done\n",
            "‚úì 385/400 done\n",
            "‚úì 386/400 done\n",
            "‚úì 387/400 done\n",
            "‚úì 388/400 done\n",
            "‚úì 389/400 done\n",
            "‚úì 390/400 done\n",
            "‚úì 391/400 done\n",
            "‚úì 392/400 done\n",
            "‚úì 393/400 done\n",
            "‚úì 394/400 done\n",
            "‚úì 395/400 done\n",
            "‚úì 396/400 done\n",
            "‚úì 397/400 done\n",
            "‚úì 398/400 done\n",
            "‚úì 399/400 done\n",
            "‚úì 400/400 done\n",
            "‚úÖ Summarization complete. Saved to: /content/pegasus_multidoc_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# TRUE MULTI-DOCUMENT SUMMARIZATION ‚Äî PEGASUS (RESUMABLE)\n",
        "# Multi-News | Hierarchical: Doc-level ‚Üí Fusion\n",
        "# =============================================================\n",
        "\n",
        "!pip install transformers rouge-score accelerate --quiet\n",
        "\n",
        "import torch, gc, os, pandas as pd\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "MODEL_NAME = \"google/pegasus-large\"\n",
        "DATA_DIR = \"/content/data_multinews\"\n",
        "SPLIT = \"test\"\n",
        "LIMIT = 600\n",
        "SAVE_PATH = f\"/content/pegasus_multidoc_{SPLIT}.csv\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ").to(device)\n",
        "model.eval()\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "src_file, tgt_file = None, None\n",
        "for root, _, files in os.walk(DATA_DIR):\n",
        "    for f in files:\n",
        "        if f.startswith(SPLIT) and \"src\" in f:\n",
        "            src_file = os.path.join(root, f)\n",
        "        if f.startswith(SPLIT) and \"tgt\" in f:\n",
        "            tgt_file = os.path.join(root, f)\n",
        "\n",
        "articles = open(src_file).read().splitlines()[:LIMIT]\n",
        "references = open(tgt_file).read().splitlines()[:LIMIT]\n",
        "\n",
        "# ---------------- RESUME SUPPORT ----------------\n",
        "start_idx = 0\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    prev_df = pd.read_csv(SAVE_PATH)\n",
        "    start_idx = len(prev_df)\n",
        "    print(f\"üîÅ Resuming from index {start_idx}\")\n",
        "\n",
        "# ---------------- HELPERS ----------------\n",
        "def split_docs(sample):\n",
        "    return [d.strip() for d in sample.split(\"|||||\") if d.strip()]\n",
        "\n",
        "def summarize(texts, max_len, min_len):\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_len,\n",
        "            min_length=min_len,\n",
        "            num_beams=4\n",
        "        )\n",
        "\n",
        "    out = tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
        "    del inputs, ids\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return out\n",
        "\n",
        "# ---------------- PIPELINE ----------------\n",
        "results = []\n",
        "\n",
        "for i in range(start_idx, len(articles)):\n",
        "    sample = articles[i]\n",
        "    docs = split_docs(sample)\n",
        "\n",
        "    # Stage 1: document-level summaries\n",
        "    doc_summaries = summarize(docs, max_len=256, min_len=60)\n",
        "\n",
        "    # Stage 2: fusion summary\n",
        "    fused_text = \" \".join(doc_summaries)\n",
        "    final_summary = summarize([fused_text], max_len=150, min_len=60)[0]\n",
        "\n",
        "    results.append({\n",
        "        \"document\": sample,\n",
        "        \"reference\": references[i],\n",
        "        \"summary\": final_summary\n",
        "    })\n",
        "\n",
        "    print(f\"‚úì {i+1}/{len(articles)} done\")\n",
        "\n",
        "    # save every sample (safe for Colab crashes)\n",
        "    pd.DataFrame(results).to_csv(\n",
        "        SAVE_PATH,\n",
        "        mode=\"a\",\n",
        "        header=not os.path.exists(SAVE_PATH),\n",
        "        index=False\n",
        "    )\n",
        "    results.clear()\n",
        "\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"‚úÖ Summarization complete. Saved to:\", SAVE_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiKUtvEsMTPQ",
        "outputId": "5289afa5-0b12-4be4-b10f-21d61829daa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Resuming from index 400\n",
            "‚úì 401/600 done\n",
            "‚úì 402/600 done\n",
            "‚úì 403/600 done\n",
            "‚úì 404/600 done\n",
            "‚úì 405/600 done\n",
            "‚úì 406/600 done\n",
            "‚úì 407/600 done\n",
            "‚úì 408/600 done\n",
            "‚úì 409/600 done\n",
            "‚úì 410/600 done\n",
            "‚úì 411/600 done\n",
            "‚úì 412/600 done\n",
            "‚úì 413/600 done\n",
            "‚úì 414/600 done\n",
            "‚úì 415/600 done\n",
            "‚úì 416/600 done\n",
            "‚úì 417/600 done\n",
            "‚úì 418/600 done\n",
            "‚úì 419/600 done\n",
            "‚úì 420/600 done\n",
            "‚úì 421/600 done\n",
            "‚úì 422/600 done\n",
            "‚úì 423/600 done\n",
            "‚úì 424/600 done\n",
            "‚úì 425/600 done\n",
            "‚úì 426/600 done\n",
            "‚úì 427/600 done\n",
            "‚úì 428/600 done\n",
            "‚úì 429/600 done\n",
            "‚úì 430/600 done\n",
            "‚úì 431/600 done\n",
            "‚úì 432/600 done\n",
            "‚úì 433/600 done\n",
            "‚úì 434/600 done\n",
            "‚úì 435/600 done\n",
            "‚úì 436/600 done\n",
            "‚úì 437/600 done\n",
            "‚úì 438/600 done\n",
            "‚úì 439/600 done\n",
            "‚úì 440/600 done\n",
            "‚úì 441/600 done\n",
            "‚úì 442/600 done\n",
            "‚úì 443/600 done\n",
            "‚úì 444/600 done\n",
            "‚úì 445/600 done\n",
            "‚úì 446/600 done\n",
            "‚úì 447/600 done\n",
            "‚úì 448/600 done\n",
            "‚úì 449/600 done\n",
            "‚úì 450/600 done\n",
            "‚úì 451/600 done\n",
            "‚úì 452/600 done\n",
            "‚úì 453/600 done\n",
            "‚úì 454/600 done\n",
            "‚úì 455/600 done\n",
            "‚úì 456/600 done\n",
            "‚úì 457/600 done\n",
            "‚úì 458/600 done\n",
            "‚úì 459/600 done\n",
            "‚úì 460/600 done\n",
            "‚úì 461/600 done\n",
            "‚úì 462/600 done\n",
            "‚úì 463/600 done\n",
            "‚úì 464/600 done\n",
            "‚úì 465/600 done\n",
            "‚úì 466/600 done\n",
            "‚úì 467/600 done\n",
            "‚úì 468/600 done\n",
            "‚úì 469/600 done\n",
            "‚úì 470/600 done\n",
            "‚úì 471/600 done\n",
            "‚úì 472/600 done\n",
            "‚úì 473/600 done\n",
            "‚úì 474/600 done\n",
            "‚úì 475/600 done\n",
            "‚úì 476/600 done\n",
            "‚úì 477/600 done\n",
            "‚úì 478/600 done\n",
            "‚úì 479/600 done\n",
            "‚úì 480/600 done\n",
            "‚úì 481/600 done\n",
            "‚úì 482/600 done\n",
            "‚úì 483/600 done\n",
            "‚úì 484/600 done\n",
            "‚úì 485/600 done\n",
            "‚úì 486/600 done\n",
            "‚úì 487/600 done\n",
            "‚úì 488/600 done\n",
            "‚úì 489/600 done\n",
            "‚úì 490/600 done\n",
            "‚úì 491/600 done\n",
            "‚úì 492/600 done\n",
            "‚úì 493/600 done\n",
            "‚úì 494/600 done\n",
            "‚úì 495/600 done\n",
            "‚úì 496/600 done\n",
            "‚úì 497/600 done\n",
            "‚úì 498/600 done\n",
            "‚úì 499/600 done\n",
            "‚úì 500/600 done\n",
            "‚úì 501/600 done\n",
            "‚úì 502/600 done\n",
            "‚úì 503/600 done\n",
            "‚úì 504/600 done\n",
            "‚úì 505/600 done\n",
            "‚úì 506/600 done\n",
            "‚úì 507/600 done\n",
            "‚úì 508/600 done\n",
            "‚úì 509/600 done\n",
            "‚úì 510/600 done\n",
            "‚úì 511/600 done\n",
            "‚úì 512/600 done\n",
            "‚úì 513/600 done\n",
            "‚úì 514/600 done\n",
            "‚úì 515/600 done\n",
            "‚úì 516/600 done\n",
            "‚úì 517/600 done\n",
            "‚úì 518/600 done\n",
            "‚úì 519/600 done\n",
            "‚úì 520/600 done\n",
            "‚úì 521/600 done\n",
            "‚úì 522/600 done\n",
            "‚úì 523/600 done\n",
            "‚úì 524/600 done\n",
            "‚úì 525/600 done\n",
            "‚úì 526/600 done\n",
            "‚úì 527/600 done\n",
            "‚úì 528/600 done\n",
            "‚úì 529/600 done\n",
            "‚úì 530/600 done\n",
            "‚úì 531/600 done\n",
            "‚úì 532/600 done\n",
            "‚úì 533/600 done\n",
            "‚úì 534/600 done\n",
            "‚úì 535/600 done\n",
            "‚úì 536/600 done\n",
            "‚úì 537/600 done\n",
            "‚úì 538/600 done\n",
            "‚úì 539/600 done\n",
            "‚úì 540/600 done\n",
            "‚úì 541/600 done\n",
            "‚úì 542/600 done\n",
            "‚úì 543/600 done\n",
            "‚úì 544/600 done\n",
            "‚úì 545/600 done\n",
            "‚úì 546/600 done\n",
            "‚úì 547/600 done\n",
            "‚úì 548/600 done\n",
            "‚úì 549/600 done\n",
            "‚úì 550/600 done\n",
            "‚úì 551/600 done\n",
            "‚úì 552/600 done\n",
            "‚úì 553/600 done\n",
            "‚úì 554/600 done\n",
            "‚úì 555/600 done\n",
            "‚úì 556/600 done\n",
            "‚úì 557/600 done\n",
            "‚úì 558/600 done\n",
            "‚úì 559/600 done\n",
            "‚úì 560/600 done\n",
            "‚úì 561/600 done\n",
            "‚úì 562/600 done\n",
            "‚úì 563/600 done\n",
            "‚úì 564/600 done\n",
            "‚úì 565/600 done\n",
            "‚úì 566/600 done\n",
            "‚úì 567/600 done\n",
            "‚úì 568/600 done\n",
            "‚úì 569/600 done\n",
            "‚úì 570/600 done\n",
            "‚úì 571/600 done\n",
            "‚úì 572/600 done\n",
            "‚úì 573/600 done\n",
            "‚úì 574/600 done\n",
            "‚úì 575/600 done\n",
            "‚úì 576/600 done\n",
            "‚úì 577/600 done\n",
            "‚úì 578/600 done\n",
            "‚úì 579/600 done\n",
            "‚úì 580/600 done\n",
            "‚úì 581/600 done\n",
            "‚úì 582/600 done\n",
            "‚úì 583/600 done\n",
            "‚úì 584/600 done\n",
            "‚úì 585/600 done\n",
            "‚úì 586/600 done\n",
            "‚úì 587/600 done\n",
            "‚úì 588/600 done\n",
            "‚úì 589/600 done\n",
            "‚úì 590/600 done\n",
            "‚úì 591/600 done\n",
            "‚úì 592/600 done\n",
            "‚úì 593/600 done\n",
            "‚úì 594/600 done\n",
            "‚úì 595/600 done\n",
            "‚úì 596/600 done\n",
            "‚úì 597/600 done\n",
            "‚úì 598/600 done\n",
            "‚úì 599/600 done\n",
            "‚úì 600/600 done\n",
            "‚úÖ Summarization complete. Saved to: /content/pegasus_multidoc_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# TRUE MULTI-DOCUMENT SUMMARIZATION ‚Äî PEGASUS (RESUMABLE)\n",
        "# Multi-News | Hierarchical: Doc-level ‚Üí Fusion\n",
        "# =============================================================\n",
        "\n",
        "!pip install transformers rouge-score accelerate --quiet\n",
        "\n",
        "import torch, gc, os, pandas as pd\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "MODEL_NAME = \"google/pegasus-large\"\n",
        "DATA_DIR = \"/content/data_multinews\"\n",
        "SPLIT = \"test\"\n",
        "LIMIT = 800\n",
        "SAVE_PATH = f\"/content/pegasus_multidoc_{SPLIT}.csv\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ").to(device)\n",
        "model.eval()\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "src_file, tgt_file = None, None\n",
        "for root, _, files in os.walk(DATA_DIR):\n",
        "    for f in files:\n",
        "        if f.startswith(SPLIT) and \"src\" in f:\n",
        "            src_file = os.path.join(root, f)\n",
        "        if f.startswith(SPLIT) and \"tgt\" in f:\n",
        "            tgt_file = os.path.join(root, f)\n",
        "\n",
        "articles = open(src_file).read().splitlines()[:LIMIT]\n",
        "references = open(tgt_file).read().splitlines()[:LIMIT]\n",
        "\n",
        "# ---------------- RESUME SUPPORT ----------------\n",
        "start_idx = 0\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    prev_df = pd.read_csv(SAVE_PATH)\n",
        "    start_idx = len(prev_df)\n",
        "    print(f\"üîÅ Resuming from index {start_idx}\")\n",
        "\n",
        "# ---------------- HELPERS ----------------\n",
        "def split_docs(sample):\n",
        "    return [d.strip() for d in sample.split(\"|||||\") if d.strip()]\n",
        "\n",
        "def summarize(texts, max_len, min_len):\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_len,\n",
        "            min_length=min_len,\n",
        "            num_beams=4\n",
        "        )\n",
        "\n",
        "    out = tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
        "    del inputs, ids\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return out\n",
        "\n",
        "# ---------------- PIPELINE ----------------\n",
        "results = []\n",
        "\n",
        "for i in range(start_idx, len(articles)):\n",
        "    sample = articles[i]\n",
        "    docs = split_docs(sample)\n",
        "\n",
        "    # Stage 1: document-level summaries\n",
        "    doc_summaries = summarize(docs, max_len=256, min_len=60)\n",
        "\n",
        "    # Stage 2: fusion summary\n",
        "    fused_text = \" \".join(doc_summaries)\n",
        "    final_summary = summarize([fused_text], max_len=150, min_len=60)[0]\n",
        "\n",
        "    results.append({\n",
        "        \"document\": sample,\n",
        "        \"reference\": references[i],\n",
        "        \"summary\": final_summary\n",
        "    })\n",
        "\n",
        "    print(f\"‚úì {i+1}/{len(articles)} done\")\n",
        "\n",
        "    # save every sample (safe for Colab crashes)\n",
        "    pd.DataFrame(results).to_csv(\n",
        "        SAVE_PATH,\n",
        "        mode=\"a\",\n",
        "        header=not os.path.exists(SAVE_PATH),\n",
        "        index=False\n",
        "    )\n",
        "    results.clear()\n",
        "\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"‚úÖ Summarization complete. Saved to:\", SAVE_PATH)\n"
      ],
      "metadata": {
        "id": "BnUe_TbINkY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d906203a-b85f-4e17-9761-8a7f984e447c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Resuming from index 600\n",
            "‚úì 601/800 done\n",
            "‚úì 602/800 done\n",
            "‚úì 603/800 done\n",
            "‚úì 604/800 done\n",
            "‚úì 605/800 done\n",
            "‚úì 606/800 done\n",
            "‚úì 607/800 done\n",
            "‚úì 608/800 done\n",
            "‚úì 609/800 done\n",
            "‚úì 610/800 done\n",
            "‚úì 611/800 done\n",
            "‚úì 612/800 done\n",
            "‚úì 613/800 done\n",
            "‚úì 614/800 done\n",
            "‚úì 615/800 done\n",
            "‚úì 616/800 done\n",
            "‚úì 617/800 done\n",
            "‚úì 618/800 done\n",
            "‚úì 619/800 done\n",
            "‚úì 620/800 done\n",
            "‚úì 621/800 done\n",
            "‚úì 622/800 done\n",
            "‚úì 623/800 done\n",
            "‚úì 624/800 done\n",
            "‚úì 625/800 done\n",
            "‚úì 626/800 done\n",
            "‚úì 627/800 done\n",
            "‚úì 628/800 done\n",
            "‚úì 629/800 done\n",
            "‚úì 630/800 done\n",
            "‚úì 631/800 done\n",
            "‚úì 632/800 done\n",
            "‚úì 633/800 done\n",
            "‚úì 634/800 done\n",
            "‚úì 635/800 done\n",
            "‚úì 636/800 done\n",
            "‚úì 637/800 done\n",
            "‚úì 638/800 done\n",
            "‚úì 639/800 done\n",
            "‚úì 640/800 done\n",
            "‚úì 641/800 done\n",
            "‚úì 642/800 done\n",
            "‚úì 643/800 done\n",
            "‚úì 644/800 done\n",
            "‚úì 645/800 done\n",
            "‚úì 646/800 done\n",
            "‚úì 647/800 done\n",
            "‚úì 648/800 done\n",
            "‚úì 649/800 done\n",
            "‚úì 650/800 done\n",
            "‚úì 651/800 done\n",
            "‚úì 652/800 done\n",
            "‚úì 653/800 done\n",
            "‚úì 654/800 done\n",
            "‚úì 655/800 done\n",
            "‚úì 656/800 done\n",
            "‚úì 657/800 done\n",
            "‚úì 658/800 done\n",
            "‚úì 659/800 done\n",
            "‚úì 660/800 done\n",
            "‚úì 661/800 done\n",
            "‚úì 662/800 done\n",
            "‚úì 663/800 done\n",
            "‚úì 664/800 done\n",
            "‚úì 665/800 done\n",
            "‚úì 666/800 done\n",
            "‚úì 667/800 done\n",
            "‚úì 668/800 done\n",
            "‚úì 669/800 done\n",
            "‚úì 670/800 done\n",
            "‚úì 671/800 done\n",
            "‚úì 672/800 done\n",
            "‚úì 673/800 done\n",
            "‚úì 674/800 done\n",
            "‚úì 675/800 done\n",
            "‚úì 676/800 done\n",
            "‚úì 677/800 done\n",
            "‚úì 678/800 done\n",
            "‚úì 679/800 done\n",
            "‚úì 680/800 done\n",
            "‚úì 681/800 done\n",
            "‚úì 682/800 done\n",
            "‚úì 683/800 done\n",
            "‚úì 684/800 done\n",
            "‚úì 685/800 done\n",
            "‚úì 686/800 done\n",
            "‚úì 687/800 done\n",
            "‚úì 688/800 done\n",
            "‚úì 689/800 done\n",
            "‚úì 690/800 done\n",
            "‚úì 691/800 done\n",
            "‚úì 692/800 done\n",
            "‚úì 693/800 done\n",
            "‚úì 694/800 done\n",
            "‚úì 695/800 done\n",
            "‚úì 696/800 done\n",
            "‚úì 697/800 done\n",
            "‚úì 698/800 done\n",
            "‚úì 699/800 done\n",
            "‚úì 700/800 done\n",
            "‚úì 701/800 done\n",
            "‚úì 702/800 done\n",
            "‚úì 703/800 done\n",
            "‚úì 704/800 done\n",
            "‚úì 705/800 done\n",
            "‚úì 706/800 done\n",
            "‚úì 707/800 done\n",
            "‚úì 708/800 done\n",
            "‚úì 709/800 done\n",
            "‚úì 710/800 done\n",
            "‚úì 711/800 done\n",
            "‚úì 712/800 done\n",
            "‚úì 713/800 done\n",
            "‚úì 714/800 done\n",
            "‚úì 715/800 done\n",
            "‚úì 716/800 done\n",
            "‚úì 717/800 done\n",
            "‚úì 718/800 done\n",
            "‚úì 719/800 done\n",
            "‚úì 720/800 done\n",
            "‚úì 721/800 done\n",
            "‚úì 722/800 done\n",
            "‚úì 723/800 done\n",
            "‚úì 724/800 done\n",
            "‚úì 725/800 done\n",
            "‚úì 726/800 done\n",
            "‚úì 727/800 done\n",
            "‚úì 728/800 done\n",
            "‚úì 729/800 done\n",
            "‚úì 730/800 done\n",
            "‚úì 731/800 done\n",
            "‚úì 732/800 done\n",
            "‚úì 733/800 done\n",
            "‚úì 734/800 done\n",
            "‚úì 735/800 done\n",
            "‚úì 736/800 done\n",
            "‚úì 737/800 done\n",
            "‚úì 738/800 done\n",
            "‚úì 739/800 done\n",
            "‚úì 740/800 done\n",
            "‚úì 741/800 done\n",
            "‚úì 742/800 done\n",
            "‚úì 743/800 done\n",
            "‚úì 744/800 done\n",
            "‚úì 745/800 done\n",
            "‚úì 746/800 done\n",
            "‚úì 747/800 done\n",
            "‚úì 748/800 done\n",
            "‚úì 749/800 done\n",
            "‚úì 750/800 done\n",
            "‚úì 751/800 done\n",
            "‚úì 752/800 done\n",
            "‚úì 753/800 done\n",
            "‚úì 754/800 done\n",
            "‚úì 755/800 done\n",
            "‚úì 756/800 done\n",
            "‚úì 757/800 done\n",
            "‚úì 758/800 done\n",
            "‚úì 759/800 done\n",
            "‚úì 760/800 done\n",
            "‚úì 761/800 done\n",
            "‚úì 762/800 done\n",
            "‚úì 763/800 done\n",
            "‚úì 764/800 done\n",
            "‚úì 765/800 done\n",
            "‚úì 766/800 done\n",
            "‚úì 767/800 done\n",
            "‚úì 768/800 done\n",
            "‚úì 769/800 done\n",
            "‚úì 770/800 done\n",
            "‚úì 771/800 done\n",
            "‚úì 772/800 done\n",
            "‚úì 773/800 done\n",
            "‚úì 774/800 done\n",
            "‚úì 775/800 done\n",
            "‚úì 776/800 done\n",
            "‚úì 777/800 done\n",
            "‚úì 778/800 done\n",
            "‚úì 779/800 done\n",
            "‚úì 780/800 done\n",
            "‚úì 781/800 done\n",
            "‚úì 782/800 done\n",
            "‚úì 783/800 done\n",
            "‚úì 784/800 done\n",
            "‚úì 785/800 done\n",
            "‚úì 786/800 done\n",
            "‚úì 787/800 done\n",
            "‚úì 788/800 done\n",
            "‚úì 789/800 done\n",
            "‚úì 790/800 done\n",
            "‚úì 791/800 done\n",
            "‚úì 792/800 done\n",
            "‚úì 793/800 done\n",
            "‚úì 794/800 done\n",
            "‚úì 795/800 done\n",
            "‚úì 796/800 done\n",
            "‚úì 797/800 done\n",
            "‚úì 798/800 done\n",
            "‚úì 799/800 done\n",
            "‚úì 800/800 done\n",
            "‚úÖ Summarization complete. Saved to: /content/pegasus_multidoc_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# TRUE MULTI-DOCUMENT SUMMARIZATION ‚Äî PEGASUS (RESUMABLE)\n",
        "# Multi-News | Hierarchical: Doc-level ‚Üí Fusion\n",
        "# =============================================================\n",
        "\n",
        "!pip install transformers rouge-score accelerate --quiet\n",
        "\n",
        "import torch, gc, os, pandas as pd\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "MODEL_NAME = \"google/pegasus-large\"\n",
        "DATA_DIR = \"/content/data_multinews\"\n",
        "SPLIT = \"test\"\n",
        "LIMIT = 1000\n",
        "SAVE_PATH = f\"/content/pegasus_multidoc_{SPLIT}.csv\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ").to(device)\n",
        "model.eval()\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "src_file, tgt_file = None, None\n",
        "for root, _, files in os.walk(DATA_DIR):\n",
        "    for f in files:\n",
        "        if f.startswith(SPLIT) and \"src\" in f:\n",
        "            src_file = os.path.join(root, f)\n",
        "        if f.startswith(SPLIT) and \"tgt\" in f:\n",
        "            tgt_file = os.path.join(root, f)\n",
        "\n",
        "articles = open(src_file).read().splitlines()[:LIMIT]\n",
        "references = open(tgt_file).read().splitlines()[:LIMIT]\n",
        "\n",
        "# ---------------- RESUME SUPPORT ----------------\n",
        "start_idx = 0\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    prev_df = pd.read_csv(SAVE_PATH)\n",
        "    start_idx = len(prev_df)\n",
        "    print(f\"üîÅ Resuming from index {start_idx}\")\n",
        "\n",
        "# ---------------- HELPERS ----------------\n",
        "def split_docs(sample):\n",
        "    return [d.strip() for d in sample.split(\"|||||\") if d.strip()]\n",
        "\n",
        "def summarize(texts, max_len, min_len):\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_len,\n",
        "            min_length=min_len,\n",
        "            num_beams=4\n",
        "        )\n",
        "\n",
        "    out = tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
        "    del inputs, ids\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return out\n",
        "\n",
        "# ---------------- PIPELINE ----------------\n",
        "results = []\n",
        "\n",
        "for i in range(start_idx, len(articles)):\n",
        "    sample = articles[i]\n",
        "    docs = split_docs(sample)\n",
        "\n",
        "    # Stage 1: document-level summaries\n",
        "    doc_summaries = summarize(docs, max_len=256, min_len=60)\n",
        "\n",
        "    # Stage 2: fusion summary\n",
        "    fused_text = \" \".join(doc_summaries)\n",
        "    final_summary = summarize([fused_text], max_len=150, min_len=60)[0]\n",
        "\n",
        "    results.append({\n",
        "        \"document\": sample,\n",
        "        \"reference\": references[i],\n",
        "        \"summary\": final_summary\n",
        "    })\n",
        "\n",
        "    print(f\"‚úì {i+1}/{len(articles)} done\")\n",
        "\n",
        "    # save every sample (safe for Colab crashes)\n",
        "    pd.DataFrame(results).to_csv(\n",
        "        SAVE_PATH,\n",
        "        mode=\"a\",\n",
        "        header=not os.path.exists(SAVE_PATH),\n",
        "        index=False\n",
        "    )\n",
        "    results.clear()\n",
        "\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"‚úÖ Summarization complete. Saved to:\", SAVE_PATH)\n"
      ],
      "metadata": {
        "id": "yNqYrhg2NnxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6aa90c-8170-4092-9dd6-607ad92dfe09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Resuming from index 800\n",
            "‚úì 801/1000 done\n",
            "‚úì 802/1000 done\n",
            "‚úì 803/1000 done\n",
            "‚úì 804/1000 done\n",
            "‚úì 805/1000 done\n",
            "‚úì 806/1000 done\n",
            "‚úì 807/1000 done\n",
            "‚úì 808/1000 done\n",
            "‚úì 809/1000 done\n",
            "‚úì 810/1000 done\n",
            "‚úì 811/1000 done\n",
            "‚úì 812/1000 done\n",
            "‚úì 813/1000 done\n",
            "‚úì 814/1000 done\n",
            "‚úì 815/1000 done\n",
            "‚úì 816/1000 done\n",
            "‚úì 817/1000 done\n",
            "‚úì 818/1000 done\n",
            "‚úì 819/1000 done\n",
            "‚úì 820/1000 done\n",
            "‚úì 821/1000 done\n",
            "‚úì 822/1000 done\n",
            "‚úì 823/1000 done\n",
            "‚úì 824/1000 done\n",
            "‚úì 825/1000 done\n",
            "‚úì 826/1000 done\n",
            "‚úì 827/1000 done\n",
            "‚úì 828/1000 done\n",
            "‚úì 829/1000 done\n",
            "‚úì 830/1000 done\n",
            "‚úì 831/1000 done\n",
            "‚úì 832/1000 done\n",
            "‚úì 833/1000 done\n",
            "‚úì 834/1000 done\n",
            "‚úì 835/1000 done\n",
            "‚úì 836/1000 done\n",
            "‚úì 837/1000 done\n",
            "‚úì 838/1000 done\n",
            "‚úì 839/1000 done\n",
            "‚úì 840/1000 done\n",
            "‚úì 841/1000 done\n",
            "‚úì 842/1000 done\n",
            "‚úì 843/1000 done\n",
            "‚úì 844/1000 done\n",
            "‚úì 845/1000 done\n",
            "‚úì 846/1000 done\n",
            "‚úì 847/1000 done\n",
            "‚úì 848/1000 done\n",
            "‚úì 849/1000 done\n",
            "‚úì 850/1000 done\n",
            "‚úì 851/1000 done\n",
            "‚úì 852/1000 done\n",
            "‚úì 853/1000 done\n",
            "‚úì 854/1000 done\n",
            "‚úì 855/1000 done\n",
            "‚úì 856/1000 done\n",
            "‚úì 857/1000 done\n",
            "‚úì 858/1000 done\n",
            "‚úì 859/1000 done\n",
            "‚úì 860/1000 done\n",
            "‚úì 861/1000 done\n",
            "‚úì 862/1000 done\n",
            "‚úì 863/1000 done\n",
            "‚úì 864/1000 done\n",
            "‚úì 865/1000 done\n",
            "‚úì 866/1000 done\n",
            "‚úì 867/1000 done\n",
            "‚úì 868/1000 done\n",
            "‚úì 869/1000 done\n",
            "‚úì 870/1000 done\n",
            "‚úì 871/1000 done\n",
            "‚úì 872/1000 done\n",
            "‚úì 873/1000 done\n",
            "‚úì 874/1000 done\n",
            "‚úì 875/1000 done\n",
            "‚úì 876/1000 done\n",
            "‚úì 877/1000 done\n",
            "‚úì 878/1000 done\n",
            "‚úì 879/1000 done\n",
            "‚úì 880/1000 done\n",
            "‚úì 881/1000 done\n",
            "‚úì 882/1000 done\n",
            "‚úì 883/1000 done\n",
            "‚úì 884/1000 done\n",
            "‚úì 885/1000 done\n",
            "‚úì 886/1000 done\n",
            "‚úì 887/1000 done\n",
            "‚úì 888/1000 done\n",
            "‚úì 889/1000 done\n",
            "‚úì 890/1000 done\n",
            "‚úì 891/1000 done\n",
            "‚úì 892/1000 done\n",
            "‚úì 893/1000 done\n",
            "‚úì 894/1000 done\n",
            "‚úì 895/1000 done\n",
            "‚úì 896/1000 done\n",
            "‚úì 897/1000 done\n",
            "‚úì 898/1000 done\n",
            "‚úì 899/1000 done\n",
            "‚úì 900/1000 done\n",
            "‚úì 901/1000 done\n",
            "‚úì 902/1000 done\n",
            "‚úì 903/1000 done\n",
            "‚úì 904/1000 done\n",
            "‚úì 905/1000 done\n",
            "‚úì 906/1000 done\n",
            "‚úì 907/1000 done\n",
            "‚úì 908/1000 done\n",
            "‚úì 909/1000 done\n",
            "‚úì 910/1000 done\n",
            "‚úì 911/1000 done\n",
            "‚úì 912/1000 done\n",
            "‚úì 913/1000 done\n",
            "‚úì 914/1000 done\n",
            "‚úì 915/1000 done\n",
            "‚úì 916/1000 done\n",
            "‚úì 917/1000 done\n",
            "‚úì 918/1000 done\n",
            "‚úì 919/1000 done\n",
            "‚úì 920/1000 done\n",
            "‚úì 921/1000 done\n",
            "‚úì 922/1000 done\n",
            "‚úì 923/1000 done\n",
            "‚úì 924/1000 done\n",
            "‚úì 925/1000 done\n",
            "‚úì 926/1000 done\n",
            "‚úì 927/1000 done\n",
            "‚úì 928/1000 done\n",
            "‚úì 929/1000 done\n",
            "‚úì 930/1000 done\n",
            "‚úì 931/1000 done\n",
            "‚úì 932/1000 done\n",
            "‚úì 933/1000 done\n",
            "‚úì 934/1000 done\n",
            "‚úì 935/1000 done\n",
            "‚úì 936/1000 done\n",
            "‚úì 937/1000 done\n",
            "‚úì 938/1000 done\n",
            "‚úì 939/1000 done\n",
            "‚úì 940/1000 done\n",
            "‚úì 941/1000 done\n",
            "‚úì 942/1000 done\n",
            "‚úì 943/1000 done\n",
            "‚úì 944/1000 done\n",
            "‚úì 945/1000 done\n",
            "‚úì 946/1000 done\n",
            "‚úì 947/1000 done\n",
            "‚úì 948/1000 done\n",
            "‚úì 949/1000 done\n",
            "‚úì 950/1000 done\n",
            "‚úì 951/1000 done\n",
            "‚úì 952/1000 done\n",
            "‚úì 953/1000 done\n",
            "‚úì 954/1000 done\n",
            "‚úì 955/1000 done\n",
            "‚úì 956/1000 done\n",
            "‚úì 957/1000 done\n",
            "‚úì 958/1000 done\n",
            "‚úì 959/1000 done\n",
            "‚úì 960/1000 done\n",
            "‚úì 961/1000 done\n",
            "‚úì 962/1000 done\n",
            "‚úì 963/1000 done\n",
            "‚úì 964/1000 done\n",
            "‚úì 965/1000 done\n",
            "‚úì 966/1000 done\n",
            "‚úì 967/1000 done\n",
            "‚úì 968/1000 done\n",
            "‚úì 969/1000 done\n",
            "‚úì 970/1000 done\n",
            "‚úì 971/1000 done\n",
            "‚úì 972/1000 done\n",
            "‚úì 973/1000 done\n",
            "‚úì 974/1000 done\n",
            "‚úì 975/1000 done\n",
            "‚úì 976/1000 done\n",
            "‚úì 977/1000 done\n",
            "‚úì 978/1000 done\n",
            "‚úì 979/1000 done\n",
            "‚úì 980/1000 done\n",
            "‚úì 981/1000 done\n",
            "‚úì 982/1000 done\n",
            "‚úì 983/1000 done\n",
            "‚úì 984/1000 done\n",
            "‚úì 985/1000 done\n",
            "‚úì 986/1000 done\n",
            "‚úì 987/1000 done\n",
            "‚úì 988/1000 done\n",
            "‚úì 989/1000 done\n",
            "‚úì 990/1000 done\n",
            "‚úì 991/1000 done\n",
            "‚úì 992/1000 done\n",
            "‚úì 993/1000 done\n",
            "‚úì 994/1000 done\n",
            "‚úì 995/1000 done\n",
            "‚úì 996/1000 done\n",
            "‚úì 997/1000 done\n",
            "‚úì 998/1000 done\n",
            "‚úì 999/1000 done\n",
            "‚úì 1000/1000 done\n",
            "‚úÖ Summarization complete. Saved to: /content/pegasus_multidoc_test.csv\n"
          ]
        }
      ]
    }
  ]
}